Chat GPT

Additional preprocessing steps, such as tokenization and batching, are important for preparing the data for training a model like ChatGPT. These steps are typically performed using the map and shuffle functions in TensorFlow 2 and PyTorch.

Tokenization: The process of breaking down the text into individual tokens, such as words or subwords. This is important because the model expects the input to be in the form of tokens, not raw text. Tokenization can be done using various techniques such as splitting the text by whitespaces or using a pre-trained tokenizer.

Batching: The process of grouping the data into smaller chunks or batches. This is important because the model is typically trained on a batch of data at a time, rather than on the entire dataset. Batching allows the model to process multiple examples at once, which can speed up the training process.

Shuffling: The process of randomly shuffling the data before each training epoch. This is important because it helps prevent the model from overfitting to the order of the data. By shuffling the data, each training epoch will be exposed to a different order of examples, which can help the model generalize better.

It's important to note that the tokenization, batching, and shuffling steps can be performed using different functions and libraries depending on the framework you use. The above script is just an example and it can be modified to use different tokenization method or different batch size.




The English Wikipedia dump is a large dataset containing the entire English Wikipedia in an XML format. The dump is released regularly and can be downloaded from the following website:

https://dumps.wikimedia.org/enwiki/

You will have to select the most recent available dump and download the corresponding file. The dump is usually compressed using bz2 or 7z format, you will have to decompress the file to access the data.

It's important to note that the file size can reach several gigabytes, so it may take some time to download the data. Also, the data is in XML format, so you may need to parse the data and perform some preprocessing before using it to train your model.




Generating 13GB/s IOs on storage media and DRAM for a ChatGPT model would be a very demanding task and would likely require a large and complex dataset as well as a powerful machine with a lot of memory and storage resources.

For the dataset, you can use a large corpus of text data such as books, articles or web pages. For example, you can use the English Wikipedia dump that contains several gigabytes of text data.

For the model, you can use a large pre-trained model such as GPT-3 or a similar transformer-based model. These models typically have billions of parameters and require a significant amount of memory and storage.

During training, the data is loaded into the DRAM in batches, where the model processes it to update the model's parameters. The data is also stored on the storage media, such as NVMe SSDs, and is read and written to frequently during the training process.

It's important to note that generating 13GB/s IOs is a very demanding task, and even with a large dataset and a powerful model, it may not be possible to achieve this level of performance on a single machine. You may need to use a cluster of machines or cloud resources to generate this level of IOs.

It's also important to note that generating 13GB/s IOs is a very demanding task and requires a large amount of memory and storage resources, as well as a powerful GPU to process the data quickly enough. It's not a common requirement for most use cases and it's important to consider the cost and feasibility before attempting this task.




=====
mount -t lustre 192.168.61.80@tcp:/lustre /mnt/lustre
======


import argparse
import subprocess
import os, fnmatch, shutil
import re
import parser
from random import seed
from random import randint
from pathlib import Path
from datetime import datetime
from paramiko import client
from distutils.dir_util import copy_tree
from scp import SCPClient, SCPException
import json
from time import sleep
from lib.remote import *
import lib.application as app
import apps.imagenet.imagenet as img

# silent =" 2>/dev/null || true"

# PROJ_DIR = os.path.realpath(os.path.join(__file__, os.pardir))
# APPS_CONF_DIR = os.path.join(PROJ_DIR, "apps")


# seed()

def parse_args():
    parser = argparse.ArgumentParser()

    def csv_str(str1):
        return str1.split(',')
    parser.add_argument('-t','--testname',dest='tname',default='test',help='Testname | e.g. mytest')
    parser.add_argument('-n','--num',dest='num',default=1,type=int,help='Number of tenants | e.g. 2')
    # parser.add_argument('-m','--master',dest='m_id',required=True,type=csv_str,help='Server | e.g. kvm-1,kvm-2')
    parser.add_argument('-s','--servers',dest='s_id',required=True,type=csv_str,help='Server | e.g. kvm-1,kvm-2')
    parser.add_argument('-c','--clients',dest='c_id',required=True,type=csv_str,help='Clients | e.g. kvm-1,kvm-2')
    parser.add_argument('-d','--device',dest='d_id',default='sdb,sdb,sdb,sdb',type=csv_str,help='Storage | e.g. sdb,sdb')
    parser.add_argument('-mt','--mount',dest='mnt',required=False,type=str,help='Dataset Mountpath | e.g. /mnt/lustre/cosmo')
    parser.add_argument('-g','--gpu',dest='gpu',required=False,help='No. of GPUs')
    parser.add_argument('-proc','--procs',dest='procs',required=False,help='No. of processes or tasks')
    parser.add_argument('-th','--threads',dest='th',required=False,help='Number of threads for Pre-processing')
    parser.add_argument('-ct','--ctrace',dest='ctrace',action='store_true',help='Run Ctrace if flag enabled')
    
    # TODO currently shared account id/password. it needs to change to multiple account and password
    parser.add_argument('-u','--user',dest='user',default='root',help='DevClous Autorun server account | e.g. root')
    parser.add_argument('-p','--password',dest='password',default='password',help='DevCloud Autorun server account | e.g. password')

    args = parser.parse_args()
    return args


def main():
    # Get working directory(absolute)
    cwd = os.getcwd()

    # Collect arguments
    args = parse_args()
    dev = args.d_id
    node = len(args.c_id)
    clients = " ".join(args.c_id)
    gpu = args.gpu
    procs = args.procs
    th = args.th
    master = '192.168.61.88'
    

    
    # Initialize conn in servers
    brokers = [RemoteConn(svr) for svr in args.s_id]
    #Initialize conn in clients
    client_conn = [RemoteConn(svr) for svr in args.c_id]
    # Initialize conn in master
    master_conn = RemoteConn(master)
    
    
    # Init variables
    respath = init_test(args.tname, cwd, f'{cwd}/apps/imagenet/yaml')
    respath_imagenet = mkdir_p(respath,'stage1')
    print("respath :", respath)
    yamlpath = os.path.join(respath, 'yaml')
    ns = f'{args.tname}-imagenet'

    
    # Imagenet Pre-run setup
    pre_imagenet_mkdir = img.Imagenet('imagenet', clients, ns, yamlpath, master_conn)
    pre_imagenet_mkdir.imagenet_load_bm(master_conn)
    
    # Start Imagenet Pre-Processing - Stage 1
    print(f'Loading benchmark.... Load Phase')
    # load_imagenet = img.Imagenet('imagenet', clients, ns, yamlpath)
    # load_imagenet.imagenet_load_bm(master_conn)
    

if __name__=='__main__':
    main()


========

새해 복 많이 받으시고, 부디 건강하시고, 가족 모두 행복한 한해 되시길 기원합니다.

=====================
[root@bright88 democratic-csi]# cat examples/nfs-client.yaml
# driver only works with 1.16+
csiDriver:
  # should be globally unique for a given cluster
  name: "org.democratic-csi.nfs-client"

storageClasses:
- name: nfs-client
  defaultClass: false
  reclaimPolicy: Delete
  volumeBindingMode: Immediate
  allowVolumeExpansion: false
  parameters:
    fsType: nfs

  mountOptions:
  - noatime
  - nfsvers=3
  secrets:
    provisioner-secret:
    controller-publish-secret:
    node-stage-secret:
    node-publish-secret:
    controller-expand-secret:

# if your cluster supports snapshots you may enable below
volumeSnapshotClasses: []
#- name: nfs-client
#  secrets:
#    snapshotter-secret:


driver:
  config:
    # please see the most up-to-date example of the corresponding config here:
    # https://github.com/democratic-csi/democratic-csi/tree/master/examples
    # YOU MUST COPY THE DATA HERE INLINE!
    driver: nfs-client
    instance_id:
    nfs:
      shareHost: 192.168.61.89
      shareBasePath: "/mnt/lustre"
      # shareHost:shareBasePath should be mounted at this location in the controller container
      controllerBasePath: "/mnt/lustre/data"
      dirPermissionsMode: "0777"
      dirPermissionsUser: root
      dirPermissionsGroup: root
    #...

# There are 4 different approaches to installing the driver
# 1: Run the controller service separated from the node service, mount the base share into the controller pod at run time
# 2. Run the controller service separated from the node service, use an existing hostPath mount of the base share from the node in the controller pod
# 3. Run the controller service jointly with the node service, mount the base share into the node pod at run time
# 4. Run the controller service jointly with the node service, use an existing hostPath mount of the base share from the node in the node pod
#
# Uncomment the lines/sections below appropriate for your desired use-case

controller:
  enabled: true

  externalResizer:
    enabled: false

  # For Options 1 and 2
  strategy: deployment

  # For Option 1
  #hostNetwork: true
  #hostIPC: true

  # For Options 3 and 4
  #strategy: node


# Option 1
# do this if the nodes do NOT already have the base volume mounted out-of-band from k8s
#  driver:
#    securityContext:
#      allowPrivilegeEscalation: true
#      capabilities:
#        add:
#        - SYS_ADMIN
#      privileged: true
#    lifecycle:
#      postStart:
#        exec:
#          command: ["/bin/sh", "-c", "mkdir -p <controllerBasePath>; mount <shareHost>:<shareBasePath> <controllerBasePath>"]
#      preStop:
#        exec:
#          command: ["/bin/sh","-c","umount <controllerBasePath>"]

# Option 2
# do this if all nodes DO have the base volume mounted out-of-band from k8s
  driver:
    securityContext:
      allowPrivilegeEscalation: true
      capabilities:
        add:
        - SYS_ADMIN
      privileged: true

    extraVolumeMounts:
    - name: nfs-storage
      mountPath: /mnt/lustre/data
      mountPropagation: Bidirectional
#
  extraVolumes:
  - name: nfs-storage
    hostPath:
      path: /mnt/lustre
      type: Directory


# Options 3 and 4
#node:

# Option 3
# do this if the nodes do NOT already have the base volume mounted out-of-band from k8s
#  driver:
#    lifecycle:
#      postStart:
#        exec:
#          command: ["/bin/sh", "-c", "mkdir -p <controllerBasePath>; mount <shareHost>:<shareBasePath> <controllerBasePath>"]
#      preStop:
#        exec:
#          command: ["/bin/sh","-c","umount <controllerBasePath>"]

# Option 4
# do this if all nodes DO have the base volume mounted out-of-band from k8s
#  driver:
#    extraVolumeMounts:
#    - name: nfs-storage
#      mountPath: <controllerBasePath>
#      mountPropagation: Bidirectional
#
#  extraVolumes:
#  - name: nfs-storage
#    hostPath:
#      path: /already/mounted/path/to <shareHost>:<shareBasePath>
#      type: Directory




=====
apiVersion: v1
kind: PersistentVolume
metadata:
  annotations:
    pv.kubernetes.io/provisioned-by: org.democratic-csi.nfs-client
  creationTimestamp: "2023-01-20T02:25:38Z"
  finalizers:
  - kubernetes.io/pv-protection
  name: pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26
  resourceVersion: "19593411"
  uid: d87f17ae-88bb-4a2d-8666-2fdc96adb1f6
spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 200Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: kf-claim-nfs-2
    namespace: default
    resourceVersion: "19593405"
    uid: 25f0d0be-41de-4a4d-ba21-e39088b86d26
  csi:
    controllerExpandSecretRef:
      name: controller-expand-secret-nfs-client-zfs-nfs-democratic-csi
      namespace: democratic-csi
    controllerPublishSecretRef:
      name: controller-publish-secret-nfs-client-zfs-nfs-democratic-csi
      namespace: democratic-csi
    driver: org.democratic-csi.nfs-client
    fsType: nfs
    nodePublishSecretRef:
      name: node-publish-secret-nfs-client-zfs-nfs-democratic-csi
      namespace: democratic-csi
    nodeStageSecretRef:
      name: node-stage-secret-nfs-client-zfs-nfs-democratic-csi
      namespace: democratic-csi
    volumeAttributes:
      node_attach_driver: nfs
      provisioner_driver: nfs-client
      server: 192.168.61.89
      share: /mnt/lustre/v/pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26
      storage.kubernetes.io/csiProvisionerIdentity: 1673590679347-8081-org.democratic-csi.nfs-client
    volumeHandle: pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26
  mountOptions:
  - noatime
  - nfsvers=3
  persistentVolumeReclaimPolicy: Delete
  storageClassName: nfs-client
  volumeMode: Filesystem
status:
  phase: Bound



===============================================
apiVersion: v1
kind: Pod
metadata:
  name: mlperf
spec:
  nodeSelector:
    kubernetes.io/hostname: node001
  containers:
  - name: mlperf
    image: 192.168.61.4:5000/nvidia_rn50_mx:0.2
    imagePullPolicy: IfNotPresent
    securityContext:
      privileged: true
    command: ["/bin/sh", "-c"]
    args:
        - "ls /data/imagenet/train-val-recordio-passthrough/"
          #        - "./scripts/prepare_imagenet.sh /data/imagenet/train-val-recordio-passthrough/tiny-imagenet-200 /data/imagenet/train-val-recordio-passthrough/tiny-imagenet-200 40"
    volumeMounts:
    - name: mlperf-volume
      mountPath: /data/imagenet/train-val-recordio-passthrough
  volumes:
  - name: mlperf-volume
    persistentVolumeClaim:
      claimName: kf-claim-nfs-2
  imagePullSecrets:
  - name: regcred





kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: kf-claim-nfs-2
  annotations:
    volume.beta.kubernetes.io/storage-class: "nfs-client"
spec:
  storageClassName: nfs-client
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 200Gi
  nodeSelector:
    kubernetes.io/hostname: node001







[root@bright88 test-scripts]# kubectl describe pv pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26
Name:            pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26
Labels:          <none>
Annotations:     pv.kubernetes.io/provisioned-by: org.democratic-csi.nfs-client
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    nfs-client
Status:          Bound
Claim:           default/kf-claim-nfs-2
Reclaim Policy:  Delete
Access Modes:    RWO
VolumeMode:      Filesystem
Capacity:        200Gi
Node Affinity:   <none>
Message:
Source:
    Type:              CSI (a Container Storage Interface (CSI) volume source)
    Driver:            org.democratic-csi.nfs-client
    FSType:            nfs
    VolumeHandle:      pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26
    ReadOnly:          false
    VolumeAttributes:      node_attach_driver=nfs
                           provisioner_driver=nfs-client
                           server=192.168.61.89
                           share=/mnt/lustre/v/pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26
                           storage.kubernetes.io/csiProvisionerIdentity=1673590679347-8081-org.democratic-csi.nfs-client
Events:                <none>
[root@bright88 test-scripts]# kubectl  get pvc
NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
kf-claim-nfs-2   Bound    pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26   200Gi      RWO            nfs-client     7m15s
[root@bright88 test-scripts]# kubectl describe pvc kf-claim-nfs-2
Name:          kf-claim-nfs-2
Namespace:     default
StorageClass:  nfs-client
Status:        Bound
Volume:        pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26
Labels:        <none>
Annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
               volume.beta.kubernetes.io/storage-class: nfs-client
               volume.beta.kubernetes.io/storage-provisioner: org.democratic-csi.nfs-client
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      200Gi
Access Modes:  RWO
VolumeMode:    Filesystem
Used By:       mlperf
Events:
  Type    Reason                 Age                    From                                                                                                                  Message
  ----    ------                 ----                   ----                                                                                                                  -------
  Normal  Provisioning           7m34s                  org.democratic-csi.nfs-client_zfs-nfs-democratic-csi-controller-f7557ff5d-m6wjc_d73e305d-5952-4ae2-9ba3-b9a36a67df4b  External provisioner is provisioning volume for claim "default/kf-claim-nfs-2"
  Normal  ProvisioningSucceeded  7m34s                  org.democratic-csi.nfs-client_zfs-nfs-democratic-csi-controller-f7557ff5d-m6wjc_d73e305d-5952-4ae2-9ba3-b9a36a67df4b  Successfully provisioned volume pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26
  Normal  ExternalProvisioning   7m33s (x2 over 7m33s)  persistentvolume-controller                                                                                           waiting for a volume to be created, either by external provisioner "org.democratic-csi.nfs-client" or manually created by system administrator





[root@bright88 examples]# kubectl get pods -n democratic-csi
NAME                                                READY   STATUS    RESTARTS   AGE
zfs-nfs-democratic-csi-controller-f7557ff5d-m6wjc   4/4     Running   169        7d15h
zfs-nfs-democratic-csi-node-47vf2                   4/4     Running   0          13d
zfs-nfs-democratic-csi-node-8b7pp                   4/4     Running   461        13d
zfs-nfs-democratic-csi-node-bfzpm                   4/4     Running   0          13d
zfs-nfs-democratic-csi-node-d4pqk                   4/4     Running   0          13d




driver: lustre-client
instance_id:
lustre:
  shareHost: 192.168.61.80
  shareBasePath: "/mnt/lustre"
  # shareHost:shareBasePath should be mounted at this location in the controller container
  controllerBasePath: "/storage"
  dirPermissionsMode: "0777"
  dirPermissionsUser: root
  dirPermissionsGroup: root






MountVolume.MountDevice failed for volume "pvc-7b5b1f73-d73a-4249-bda2-45524313d69a" : rpc error: code = Internal desc = {"code":32,"stdout":"","stderr":"/usr/local/bin/mount: illegal option -- o\nmount.nfs: requested NFS version or transport protocol is not supported\n","timeout":false}

==============================================

Volumes:
  mlperf-volume:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  mlperf-test-node001-pvc
    ReadOnly:   false
  mlperf-log:
    Type:          HostPath (bare host directory volume)
    Path:          /mnt/
    HostPathType:
  host-folder:
    Type:          HostPath (bare host directory volume)
    Path:          /tmp/
    HostPathType:
  kube-api-access-hmb84:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=node001
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age               From               Message
  ----     ------            ----              ----               -------
  Warning  FailedScheduling  18s               default-scheduler  0/7 nodes are available: 7 pod has unbound immediate PersistentVolumeClaims.
  Normal   Scheduled         17s               default-scheduler  Successfully assigned mlperf-test-node001/mlperf to node001
  Warning  FailedMount       1s (x6 over 17s)  kubelet            MountVolume.MountDevice failed for volume "pvc-fd4bde75-15bd-4962-b2c7-8d2232d0bfb2" : rpc error: code = Internal desc = {"code":32,"stdout":"","stderr":"/usr/local/bin/mount: illegal option -- o\nmount.nfs: requested NFS version or transport protocol is not supported\n","timeout":false}




error parsing load.yaml: error converting YAML to JSON: yaml: line 20: did not find expected key



from kubernetes import client, config

class KubernetesWorker:
    def __init__(self):
        # loading kubeconfig
        config.load_kube_config()
        # create an instance of the Kubernetes CoreV1Api
        self.api_instance = client.CoreV1Api()

    def show_nodes(self):
        nodes = self.api_instance.list_node()
        for node in nodes.items:
            print(node.metadata.name)

# Usage
worker = KubernetesWorker()
worker.show_nodes()












slurmstepd: error: pyxis: container start failed with error code: 1
slurmstepd: error: pyxis: printing enroot log file:
slurmstepd: error: pyxis:     /usr/bin/enroot: line 44: HOME: unbound variable
slurmstepd: error: pyxis:     [ERROR] Command not found: nvidia-container-cli, see https://github.com/NVIDIA/libnvidia-container
slurmstepd: error: pyxis:     [ERROR] /etc/enroot/hooks.d/98-nvidia.sh exited with return code 1















[root@bright88 mxnet]# srun --mpi=pmix_v3 -N 1 --ntasks=16  -w node001 --container-image=192.168.61.4:5000#/cosmoflow-nvidia:0.4 --container-name=cosmoflow-preprocess --container-workdir=/mnt/mxnet --container-mounts=/mnt/lustre:/mnt bash tools/init_datasets.sh  /mnt/Cosmo-Small /mnt/processed 16









pyxis: imported docker image: 192.168.61.4:5000#/cosmoflow-nvidia:0.4
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
[node001:110631] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110633] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110638] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110636] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110640] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110639] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110632] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110642] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110641] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110634] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110644] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110635] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110643] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110630] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110629] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110637] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
2023-01-11 15:37:23.068623: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.068624: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.070205: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.072718: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.080399: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.085484: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.085311: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.086233: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.086883: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.088807: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.092000: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.094736: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.097027: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.098812: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.220895: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.228529: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:32.585366: F tensorflow/core/platform/statusor.cc:33] Attempting to fetch value instead of handling error INTERNAL: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 85197979648
[node001:110642] *** Process received signal ***
[node001:110642] Signal: Aborted (6)
[node001:110642] Signal code:  (-6)
[node001:110642] [ 0] /usr/lib/x86_64-linux-gnu/libc.so.6(+0x46210)[0x155555370210]
[node001:110642] [ 1] /usr/lib/x86_64-linux-gnu/libc.so.6(gsignal+0xcb)[0x15555537018b]
[node001:110642] [ 2] /usr/lib/x86_64-linux-gnu/libc.so.6(abort+0x12b)[0x15555534f859]
[node001:110642] [ 3] /usr/local/lib/python3.8/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(+0x10a461a4)[0x1555331541a4]
[node001:110642] [ 4] /usr/local/lib/python3.8/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow17internal_statusor6Helper5CrashERKNS_6StatusE+0x4e)[0x1555331477ce]
[node001:110642] [ 5] /usr/local/lib/python3.8/dist-packages/tensorflow/python/../libtensorflow_framework.so.2(+0x9b3f0b)[0x155521133f0b]
[node001:110642] [ 6] /usr/local/lib/python3.8/dist-packages/tensorflow/python/../libtensorflow_framework.so.2(_ZN10tensorflow20BaseGPUDeviceFactory13CreateDevicesERKNS_14SessionOptionsERKSsPSt6vectorISt10unique_ptrINS_6DeviceESt14default_deleteIS8_EESaISB_EE+0x77b)[0x15552113a95b]
[node001:110642] [ 7] /usr/local/lib/python3.8/dist-packages/tensorflow/python/../libtensorflow_framework.so.2(_ZN10tensorflow13DeviceFactory10AddDevicesERKNS_14SessionOptionsERKSsPSt6vectorISt10unique_ptrINS_6DeviceESt14default_deleteIS8_EESaISB_EE+0xad)[0x155520ea5bfd]
[node001:110642] [ 8] /usr/local/lib/python3.8/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(TFE_NewContext+0x70)[0x155526cb6050]
[node001:110642] [ 9] /usr/local/lib/python3.8/dist-packages/tensorflow/python/_pywrap_tfe.so(+0x598f5)[0x15551fb948f5]
[node001:110642] [10] /usr/local/lib/python3.8/dist-packages/tensorflow/python/_pywrap_tfe.so(+0x55141)[0x15551fb90141]
[node001:110642] [11] python3(PyCFunction_Call+0x59)[0x5f2cc9]
[node001:110642] [12] python3(_PyObject_MakeTpCall+0x150)[0x5f3010]
[node001:110642] [13] python3(_PyEval_EvalFrameDefault+0x5d43)[0x5700f3]
[node001:110642] [14] python3(_PyFunction_Vectorcall+0x1b6)[0x5f5956]
[node001:110642] [15] python3(_PyEval_EvalFrameDefault+0x906)[0x56acb6]
[node001:110642] [16] python3(_PyEval_EvalCodeWithName+0x26a)[0x568d9a]
[node001:110642] [17] python3(_PyFunction_Vectorcall+0x393)[0x5f5b33]
[node001:110642] [18] python3(_PyEval_EvalFrameDefault+0x72f)[0x56aadf]
[node001:110642] [19] python3(_PyFunction_Vectorcall+0x1b6)[0x5f5956]
[node001:110642] [20] python3(_PyEval_EvalFrameDefault+0x72f)[0x56aadf]
[node001:110642] [21] python3(_PyEval_EvalCodeWithName+0x26a)[0x568d9a]
[node001:110642] [22] python3(_PyFunction_Vectorcall+0x393)[0x5f5b33]
[node001:110642] [23] python3(_PyEval_EvalFrameDefault+0x18eb)[0x56bc9b]
[node001:110642] [24] python3(_PyEval_EvalCodeWithName+0x26a)[0x568d9a]
[node001:110642] [25] python3(_PyFunction_Vectorcall+0x393)[0x5f5b33]
[node001:110642] [26] python3(_PyEval_EvalFrameDefault+0x18eb)[0x56bc9b]
[node001:110642] [27] python3(_PyEval_EvalCodeWithName+0x26a)[0x568d9a]
[node001:110642] [28] python3(_PyFunction_Vectorcall+0x393)[0x5f5b33]
[node001:110642] [29] python3(_PyEval_EvalFrameDefault+0x18eb)[0x56bc9b]
[node001:110642] *** End of error message ***
tools/init_datasets.sh: line 7: 110642 Aborted                 (core dumped) python3 -m tools.convert_tfrecord_to_numpy -i ${DATA_SRC_DIR}/train -o ${DATA_DST_DIR}/train -c GZIP -p ${NUM_PROC}
Train set done
slurmstepd: error:  mpi/pmix_v3: _errhandler: node001 [0]: pmixp_client_v2.c:211: Error handler invoked: status = -25, source = [slurm.pmix.259.0:7]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 259.0 ON node001 CANCELLED AT 2023-01-11T15:37:34 ***
slurmstepd: error:  mpi/pmix_v3: _errhandler: node001 [0]: pmixp_client_v2.c:211: Error handler invoked: status = -25, source = [slurm.pmix.259.0:9]
slurmstepd: error:  mpi/pmix_v3: _errhandler: node001 [0]: pmixp_client_v2.c:211: Error handler invoked: status = -25, source = [slurm.pmix.259.0:10]
srun: error: node001: tasks 0-6,8-15: Killed
