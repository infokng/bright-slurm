

[1,1]<stderr>:/usr/local/lib/python3.8/dist-packages/nvidia/dali/plugin/mxnet.py:95: Warning: Please do not use `fill_last_batch` and use `last_batch_policy`                            instead.
[1,1]<stderr>:  _DaliBaseIterator.__init__(self,
[1,1]<stderr>:2023-02-23 02:06:24,506:WARNING: DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
[1,1]<stderr>:2023-02-23 02:06:24,506:INFO: Starting epoch 0
[1,2]<stderr>:/workspace/rn50/dali.py:83: DeprecationWarning: The argument ``output_dtype`` is a deprecated alias for ``dtype``. Use ``dtype`` instead.
[1,2]<stderr>:  self.cmnp = ops.CropMirrorNormalize(device="gpu",
[1,2]<stderr>:/workspace/rn50/dali.py:83: DeprecationWarning: The argument ``image_type`` is no longer used and will be removed in a future release.
[1,2]<stderr>:  self.cmnp = ops.CropMirrorNormalize(device="gpu",
[1,2]<stderr>:/workspace/rn50/dali.py:128: DeprecationWarning: The argument ``output_dtype`` is a deprecated alias for ``dtype``. Use ``dtype`` instead.
[1,2]<stderr>:  self.cmnp = ops.CropMirrorNormalize(device="gpu",
[1,2]<stderr>:/workspace/rn50/dali.py:128: DeprecationWarning: The argument ``image_type`` is no longer used and will be removed in a future release.
[1,2]<stderr>:  self.cmnp = ops.CropMirrorNormalize(device="gpu",
[1,2]<stderr>:/usr/local/lib/python3.8/dist-packages/nvidia/dali/ops.py:649: DeprecationWarning: WARNING: `coin_flip` is now deprecated. Use `random.coin_flip` instead.
[1,2]<stderr>:  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
[1,2]<stderr>:/usr/local/lib/python3.8/dist-packages/nvidia/dali/ops.py:649: DeprecationWarning: WARNING: `mxnet_reader` is now deprecated. Use `readers.mxnet` instead.
[1,2]<stderr>:In DALI 1.0 all readers were moved into a dedicated :mod:`~nvidia.dali.fn.readers`
[1,2]<stderr>:submodule and renamed to follow a common pattern. This is a placeholder operator with identical
[1,2]<stderr>:functionality to allow for backward compatibility.
[1,2]<stderr>:  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
[1,2]<stderr>:/usr/local/lib/python3.8/dist-packages/nvidia/dali/ops.py:649: DeprecationWarning: WARNING: `image_decoder_random_crop` is now deprecated. Use `decoders.image_random_crop` instead.
[1,2]<stderr>:In DALI 1.0 all decoders were moved into a dedicated :mod:`~nvidia.dali.fn.decoders`
[1,2]<stderr>:submodule and renamed to follow a common pattern. This is a placeholder operator with identical
[1,2]<stderr>:functionality to allow for backward compatibility.
[1,2]<stderr>:  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
[1,2]<stderr>:/usr/local/lib/python3.8/dist-packages/nvidia/dali/ops.py:649: DeprecationWarning: WARNING: `mxnet_reader` is now deprecated. Use `readers.mxnet` instead.
[1,2]<stderr>:In DALI 1.0 all readers were moved into a dedicated :mod:`~nvidia.dali.fn.readers`
[1,2]<stderr>:submodule and renamed to follow a common pattern. This is a placeholder operator with identical
[1,2]<stderr>:functionality to allow for backward compatibility.
[1,2]<stderr>:  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
[1,2]<stderr>:/usr/local/lib/python3.8/dist-packages/nvidia/dali/ops.py:649: DeprecationWarning: WARNING: `image_decoder` is now deprecated. Use `decoders.image` instead.
[1,2]<stderr>:In DALI 1.0 all decoders were moved into a dedicated :mod:`~nvidia.dali.fn.decoders`
[1,2]<stderr>:submodule and renamed to follow a common pattern. This is a placeholder operator with identical
[1,2]<stderr>:functionality to allow for backward compatibility.
[1,2]<stderr>:  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
[1,2]<stderr>:/usr/local/lib/python3.8/dist-packages/nvidia/dali/plugin/base_iterator.py:192: Warning: Please set `reader_name` and don't set last_batch_padded and size manually whenever possible. This may le
ad, in some situations, to missing some samples or returning duplicated ones. Check the Sharding section of the documentation for more details.
[1,2]<stderr>:  _iterator_deprecation_warning()
[1,2]<stderr>:/usr/local/lib/python3.8/dist-packages/nvidia/dali/plugin/mxnet.py:95: Warning: Please do not use `fill_last_batch` and use `last_batch_policy`                            instead.
[1,2]<stderr>:  _DaliBaseIterator.__init__(self,
[1,2]<stderr>:2023-02-23 02:06:26,465:WARNING: DALI iterator does not support resetting while epoch is not finished.                              Ignoring...
[1,2]<stderr>:2023-02-23 02:06:26,465:INFO: Starting epoch 0
[1,0]<stderr>:/workspace/rn50/dali.py:83: DeprecationWarning: The argument ``output_dtype`` is a deprecated alias for ``dtype``. Use ``dtype`` instead.
[1,0]<stderr>:  self.cmnp = ops.CropMirrorNormalize(device="gpu",
[1,0]<stderr>:/workspace/rn50/dali.py:83: DeprecationWarning: The argument ``image_type`` is no longer used and will be removed in a future release.
[1,0]<stderr>:  self.cmnp = ops.CropMirrorNormalize(device="gpu",
[1,0]<stderr>:/workspace/rn50/dali.py:128: DeprecationWarning: The argument ``output_dtype`` is a deprecated alias for ``dtype``. Use ``dtype`` instead.
[1,0]<stderr>:  self.cmnp = ops.CropMirrorNormalize(device="gpu",
[1,0]<stderr>:/workspace/rn50/dali.py:128: DeprecationWarning: The argument ``image_type`` is no longer used and will be removed in a future release.
[1,0]<stderr>:  self.cmnp = ops.CropMirrorNormalize(device="gpu",
[1,0]<stderr>:/usr/local/lib/python3.8/dist-packages/nvidia/dali/ops.py:649: DeprecationWarning: WARNING: `coin_flip` is now deprecated. Use `random.coin_flip` instead.
[1,0]<stderr>:  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
[1,0]<stderr>:/usr/local/lib/python3.8/dist-packages/nvidia/dali/ops.py:649: DeprecationWarning: WARNING: `mxnet_reader` is now deprecated. Use `readers.mxnet` instead.
[1,0]<stderr>:In DALI 1.0 all readers were moved into a dedicated :mod:`~nvidia.dali.fn.readers`
[1,0]<stderr>:submodule and renamed to follow a common pattern. This is a placeholder operator with identical


=====================
root@mlperf:/workspace/rn50# ./runner -n 3 -b 64 --data-root /data/imagenet/train-val-recordio-passthrough/tiny-imagenet-200                                                                          [168/1298]
[1,0]<stdout>:DLL 2023-02-23 02:06:12.819128 - PARAMETER arch : resnetv15  num_layers : 50  num_groups : 32  num_classes : 1000  batchnorm_eps : 1e-05  batchnorm_mom : 0.9  fuse_bn_relu : 1  fuse_bn_add_relu
: 1  mode : train_val  seed : None  gpus : [0, 1, 2]  kv_store : horovod  dtype : float16  amp : False  batch_size : 192  num_epochs : 90  run_epochs : -1  lr : 0.192  lr_schedule : cosine  lr_factor : 0.256
 lr_steps : []  warmup_epochs : 5  optimizer : sgd  mom : 0.875  wd : 3.0517578125e-05  label_smoothing : 0.1  mixup : 0  disp_batches : 20  model_prefix : model  save_frequency : -1  begin_epoch : 0  load :
None  test_io : False  test_io_mode : train  log : log.log  dllogger_log : dllogger_log.log  workspace : ./  logdir : None  no_metrics : False  benchmark_iters : None  data_train : /data/imagenet/train-val-re
cordio-passthrough/tiny-imagenet-200/train.rec  data_train_idx : /data/imagenet/train-val-recordio-passthrough/tiny-imagenet-200/train.idx  data_val : /data/imagenet/train-val-recordio-passthrough/tiny-imagen
et-200/val.rec  data_val_idx : /data/imagenet/train-val-recordio-passthrough/tiny-imagenet-200/val.idx  data_pred : None  data_backend : dali-gpu  image_shape : [4, 224, 224]  rgb_mean : [123.68, 116.779, 103
.939]  rgb_std : [58.393, 57.12, 57.375]  input_layout : NCHW  conv_layout : NHWC  batchnorm_layout : NHWC  pooling_layout : NHWC  num_examples : 1281167  data_val_resize : 256  dali_separ_val : False  dali_t
hreads : 6  dali_validation_threads : 10  dali_prefetch_queue : 5  dali_nvjpeg_memory_padding : 256  dali_fuse_decoder : 1  dali_nvjpeg_width_hint : 5980  dali_nvjpeg_height_hint : 6430  dali_dont_use_mmap :
False  data_mxnet_threads : 40  random_crop : 0  random_mirror : 1  max_random_h : 0  max_random_s : 0  max_random_l : 0  min_random_aspect_ratio : 0.75  max_random_aspect_ratio : 1.33  max_random_rotate_angl
e : 0  max_random_shear_ratio : 0  max_random_scale : 1  min_random_scale : 1  max_random_area : 1  min_random_area : 0.05  min_crop_size : -1  max_crop_size : -1  brightness : 0  contrast : 0  saturation : 0
  pca_noise : 0  random_resized_crop : 1
[1,0]<stderr>:[02:06:18] ../src/storage/storage.cc:196: Using Pooled (Naive) StorageManager for GPU
[1,0]<stderr>:[02:06:18[1,0]<stderr>:] ../src/storage/storage.cc:196: Using Pooled (Naive) StorageManager for CPU
[1,0]<stderr>:2023-02-23 02:06:18,308:INFO: starting epoch 0
[1,2]<stderr>:[[1,2]<stderr>:02:06:18] ../src/storage/storage.cc:196: Using Pooled (Naive) StorageManager for GPU
[1,2]<stderr>:[[1,2]<stderr>:02:06:18] ../src/storage/storage.cc:196: Using Pooled (Naive) StorageManager for CPU
[1,1]<stderr>:[[1,1]<stderr>:02:06:18] ../src/storage/storage.cc:[1,1]<stderr>:196: Using Pooled (Naive) StorageManager for GPU
[1,1]<stderr>:[02:06:18] ../src/storage/storage.cc:196: Using [1,1]<stderr>:Pooled (Naive) StorageManager for CPU
[1,2]<stderr>:2023-02-23 02:06:18,355:INFO: starting epoch 0
[1,1]<stderr>:2023-02-23 02:06:18,361:INFO: starting epoch 0
[1,1]<stderr>:[mlperf:84   :0:92] Caught signal 7 (Bus error: nonexistent physical address)
[1,0]<stderr>:[mlperf:83   :0:93] Caught signal 7 (Bus error: nonexistent physical address)
[1,2]<stderr>:[mlperf:85   :0:94] Caught signal 7 (Bus error: nonexistent physical address)
[1,1]<stderr>:/workspace/rn50/dali.py:83: DeprecationWarning: The argument ``output_dtype`` is a deprecated alias for ``dtype``. Use ``dtype`` instead.
[1,1]<stderr>:  self.cmnp = ops.CropMirrorNormalize(device="gpu",
[1,1]<stderr>:/workspace/rn50/dali.py:83: DeprecationWarning: The argument ``image_type`` is no longer used and will be removed in a future release.
[1,1]<stderr>:  self.cmnp = ops.CropMirrorNormalize(device="gpu",
[1,1]<stderr>:/workspace/rn50/dali.py:128: DeprecationWarning: The argument ``output_dtype`` is a deprecated alias for ``dtype``. Use ``dtype`` instead.
[1,1]<stderr>:  self.cmnp = ops.CropMirrorNormalize(device="gpu",
[1,1]<stderr>:/workspace/rn50/dali.py:128: DeprecationWarning: The argument ``image_type`` is no longer used and will be removed in a future release.
[1,1]<stderr>:  self.cmnp = ops.CropMirrorNormalize(device="gpu",
[1,1]<stderr>:/usr/local/lib/python3.8/dist-packages/nvidia/dali/ops.py:649: DeprecationWarning: WARNING: `coin_flip` is now deprecated. Use `random.coin_flip` instead.
[1,1]<stderr>:  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
[1,1]<stderr>:/usr/local/lib/python3.8/dist-packages/nvidia/dali/ops.py:649: DeprecationWarning: WARNING: `mxnet_reader` is now deprecated. Use `readers.mxnet` instead.
[1,1]<stderr>:In DALI 1.0 all readers were moved into a dedicated :mod:`~nvidia.dali.fn.readers`
[1,1]<stderr>:submodule and renamed to follow a common pattern. This is a placeholder operator with identical
[1,1]<stderr>:functionality to allow for backward compatibility.
[1,1]<stderr>:  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
[1,1]<stderr>:/usr/local/lib/python3.8/dist-packages/nvidia/dali/ops.py:649: DeprecationWarning: WARNING: `image_decoder_random_crop` is now deprecated. Use `decoders.image_random_crop` instead.
[1,1]<stderr>:In DALI 1.0 all decoders were moved into a dedicated :mod:`~nvidia.dali.fn.decoders`
[1,1]<stderr>:submodule and renamed to follow a common pattern. This is a placeholder operator with identical
[1,1]<stderr>:functionality to allow for backward compatibility.
[1,1]<stderr>:  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
[1,1]<stderr>:/usr/local/lib/python3.8/dist-packages/nvidia/dali/ops.py:649: DeprecationWarning: WARNING: `mxnet_reader` is now deprecated. Use `readers.mxnet` instead.
[1,1]<stderr>:In DALI 1.0 all readers were moved into a dedicated :mod:`~nvidia.dali.fn.readers`
[1,1]<stderr>:submodule and renamed to follow a common pattern. This is a placeholder operator with identical
[1,1]<stderr>:functionality to allow for backward compatibility.
[1,1]<stderr>:  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
[1,1]<stderr>:/usr/local/lib/python3.8/dist-packages/nvidia/dali/ops.py:649: DeprecationWarning: WARNING: `image_decoder` is now deprecated. Use `decoders.image` instead.
[1,1]<stderr>:In DALI 1.0 all decoders were moved into a dedicated :mod:`~nvidia.dali.fn.decoders`
[1,1]<stderr>:submodule and renamed to follow a common pattern. This is a placeholder operator with identical
[1,1]<stderr>:functionality to allow for backward compatibility.
[1,1]<stderr>:  op_instances.append(_OperatorInstance(input_set, self, **kwargs))


===================
apiVersion: v1
kind: Pod
metadata:
  name: mlperf
  labels:
    devcloud: autorun
    app: mlperf
spec:
  nodeSelector:
    kubernetes.io/hostname: node001
  containers:
  - name: mlperf
    image: 192.168.61.4:5000/nvidia_rn50_mx:0.2
    imagePullPolicy: IfNotPresent
    resources:
      limits:
        nvidia.com/gpu: 3
    securityContext:
      privileged: true
    command: ["/bin/sh", "-c"]
    args:
        - "ls /data/imagenet/train-val-recordio-passthrough/ &&
           sleep 2 &&
           ./runner -n 3 -b 64 --data-root /data/imagenet/train-val-recordio-passthrough/tiny-imagenet-200"
    volumeMounts:
    - name: mlperf-volume
      mountPath: /data/imagenet/train-val-recordio-passthrough
    - mountPath: /tmp
      name: mlperf-log
  volumes:
  - name: mlperf-volume
    persistentVolumeClaim:
      claimName: mlperf-imagenet-lustre-p5510-n1-gpu-imagenet-image-pvc
  - name: mlperf-log
    hostPath:
      path: "/mnt/"
      #  - name: params-yaml
      #    hostPath:
      #      path: "/root/autorun_karan/apps/mlperf/params.yaml"
      #  - name: darshan-volume
      #    hostPath:
      #      path: "/darshan-logs"
  restartPolicy: Never
  imagePullSecrets:
  - name: regcred


===============================================
[1,2]<stderr>:[01:52:47] ../src/storage/storage.cc:196: Using Pooled (Naive) StorageManager for GPU
[1,2]<stderr>:[[1,2]<stderr>:01:52:47] ../src/storage/storage.cc:196: Using Pooled (Naive) StorageManager for CPU
[1,2]<stderr>:2023-02-23 01:52:47,610:INFO: starting epoch 0
[1,0]<stderr>:[[1,0]<stderr>:01:52:47] ../src/storage/storage.cc:196: Using Pooled (Naive) StorageManager for [1,0]<stderr>:GPU
[1,0]<stderr>:[[1,0]<stderr>:01:52:47] ../src/storage/storage.cc:196: Using Pooled (Naive) StorageManager for [1,0]<stderr>:CPU
[1,0]<stderr>:2023-02-23 01:52:47,628:INFO: starting epoch 0
[1,1]<stderr>:[[1,1]<stderr>:01:52:47] ../src/storage/storage.cc:196: Using Pooled (Naive) StorageManager for GPU
[1,1]<stderr>:[01:52:47] ../src/storage/storage.cc:196: Using Pooled (Naive) StorageManager for CPU
[1,1]<stderr>:2023-02-23 01:52:47,673:INFO: starting epoch 0
[1,1]<stderr>:[mlperf:47   :0:56] Caught signal 7 (Bus error: nonexistent physical address)
[1,0]<stderr>:[mlperf:46   :0:57] Caught signal 7 (Bus error: nonexistent physical address)


============================================
The IOT workload is designed to simulate the ingest and querying of sensor data in a time-series database. The workload generates a large volume of data that is ingested into the database, and then queries are executed to retrieve data based on time ranges and sensor types. This workload is representative of real-world scenarios where large volumes of time-stamped data are generated by sensors and need to be processed in real-time to derive insights and make decisions.

The IoT use case in TSBS is focused on simulating the ingestion of data from IoT devices, which are typically generating a high volume of data that needs to be stored for further analysis. In this use case, the focus is primarily on write operations because the data is coming in from the devices and needs to be written to the database for storage and later analysis.

However, once the data is stored in the database, it can be queried and analyzed through read operations. The absence of read operations in the benchmark doesn't mean that reads are not necessary in a real-world IoT application. It simply means that the benchmark is focusing on the specific use case of ingesting data from IoT devices at a high rate and measuring the performance of the database in handling that workload.

panic: failed to connect to `host=timescaledb user=postgres database=postgres`: tls error (server refused TLS connection)

==============================
data-source:
  file:
    location: /mnt/timescaledb-data.gz
  type: FILE
loader:
  db-specific:
    admin-db-name: postgres
    chunk-time: 12h0m0s
    create-metrics-table: true
    field-index: VALUE-TIME
    field-index-count: 0
    force-text-format: false
    host: timescaledb
    in-table-partition-tag: false
    log-batches: false
    partition-index: true
    partitions: 0
    pass: ""
    port: "5432"
    postgres: sslmode=disable
    replication-factor: 0
    time-index: true
    time-partition-index: false
    use-hypertable: true
    use-insert: false
    use-jsonb-tags: false
    user: postgres
    write-profile: ""
    write-replication-stats: ""
  runner:
    batch-size: "10000"
    channel-capacity: "0"
    db-name: benchmark
    do-abort-on-exist: false
    do-create-db: true
    do-load: true
    flow-control: false
    hash-workers: false
    insert-intervals: ""
    limit: "0"
    reporting-period: 10s
    seed: 0
    workers: "1"
  target: timescaledb


2023/02/15 08:05:27 input header in wrong format. got '%!s(uint8=31)', expected 'tags'

=======================================
Events:
  Type     Reason       Age                From               Message
  ----     ------       ----               ----               -------
  Normal   Scheduled    62s                default-scheduler  Successfully assigned default/mlperf to node001
  Warning  FailedMount  30s (x7 over 62s)  kubelet            MountVolume.MountDevice failed for volume "pvc-7d015047-7d01-45c8-8325-afd84cd9adc1" : kubernetes.io/csi: attacher.MountDevice failed to create newCsiDriverClient: driver name org.democratic-csi.lustre-client not found in the list of registered CSI drivers
[root@bright88 ~]#
[root@bright88 ~]#
[root@bright88 ~]#
[root@bright88 ~]#
[root@bright88 ~]#
[root@bright88 ~]# kubectl get pvc
NAME                                                     STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS    AGE
mlperf-imagenet-lustre-p5510-n1-gpu-imagenet-image-pvc   Bound    pvc-7d015047-7d01-45c8-8325-afd84cd9adc1   700Gi      RWO            lustre-client   18d
[root@bright88 ~]# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                                                            STORAGECLASS    REASON   AGE
pvc-3bab6160-3057-4aae-98a9-a33e385b8f29   20Gi       RWO            Delete           Released   kubeflow/minio-pvc                                               local-path               46d
pvc-51053ed8-4392-495c-9863-56e7d180a9cb   20Gi       RWO            Delete           Bound      kubeflow/minio-pvc                                               local-path               34d
pvc-7d015047-7d01-45c8-8325-afd84cd9adc1   700Gi      RWO            Delete           Bound      default/mlperf-imagenet-lustre-p5510-n1-gpu-imagenet-image-pvc   lustre-client            18d
pvc-92c61fc5-b205-4ede-b813-278783dae6b3   10Gi       RWO            Delete           Bound      istio-system/authservice-pvc                                     local-path               34d
pvc-9931da33-7830-47b3-9915-2dee91bd6d03   10Gi       RWO            Delete           Released   kubeflow/katib-mysql                                             local-path               46d
pvc-b41a8874-8268-4743-b271-163dbe7dd124   10Gi       RWO            Delete           Released   istio-system/authservice-pvc                                     local-path               46d
pvc-c72f7b19-e0d0-475f-a6be-c2355d8c9553   10Gi       RWO            Delete           Bound      kubeflow/katib-mysql                                             local-path               34d
pvc-e05b41d8-8c00-42a8-988b-9e6274fcbf1f   20Gi       RWO            Delete           Bound      kubeflow/mysql-pv-claim                                          local-path               34d
[root@bright88 ~]# vim /cm/shared/AI-ML_autorun/apps/imagenet/yaml/load-1.yaml
[root@bright88 ~]# kubectl get sc
NAME                   PROVISIONER                        RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
local-path (default)   rancher.io/local-path              Delete          Immediate           false                  48d
lustre-client          org.democratic-csi.lustre-client   Delete          Immediate           false                  24d


====================================================
apiVersion: v1
kind: Pod
metadata:
  name: mlperf
  labels:
    devcloud: autorun
    app: mlperf
spec:
  nodeSelector:
    kubernetes.io/hostname: node001
  containers:
  - name: mlperf
    image: 192.168.61.4:5000/nvidia_rn50_mx:0.2 bash
    imagePullPolicy: IfNotPresent
    securityContext:
      privileged: true
    command: ["/bin/sh", "-c"]
    args:
        - "ls /data/imagenet/train-val-recordio-passthrough/"
    - name: mlperf-volume
      mountPath: /data/imagenet/train-val-recordio-passthrough
    - mountPath: /tmp
      name: mlperf-log
  volumes:
  - name: mlperf-volume
    persistentVolumeClaim:
      claimName: mlperf-imagenet-lustre-p5510-n1-gpu-imagenet-image-pvc
  - name: mlperf-log
    hostPath:
      path: "/mnt/"
      #  - name: params-yaml
      #    hostPath:
      #      path: "/root/autorun_karan/apps/mlperf/params.yaml"
      #  - name: darshan-volume
      #    hostPath:
      #      path: "/darshan-logs"
  restartPolicy: Never
  imagePullSecrets:
  - name: regcred


============================================================================
Substituting the given values:

total_compute_time = (313532 * 500)/120/48 * 0.968

= (156766000)/5760 * 0.968

= 2732.1807407407407 * 0.968

= 2621.074444444444 seconds

So, the total_compute_time is approximately 2621.074444444444 seconds.

AUU = (5404 - 2621) / 2621 = 1.0379 or about 103.79%

+++++++++++++++++

To calculate total_compute_time, we can use the following formula:

total_compute_time = (records/file * total_files) / (simulated_accelerators * batch_size * sleep_time)

Plugging in the values, we get:

total_compute_time = (313532 * 500) / (120 * 250 * 0.968)
total_compute_time = (15767600) / (30000)
total_compute_time = 5.2592

So the total_compute_time is approximately 5.2592 seconds.

AUU = (18743 - 5.2)/5.2

AUU = 3614.042307692308

==============================================================
root@6903151fce4e:/mnt# sh benchmark.sh datagen -h
benchmark.sh: 2: Bad substitution
benchmark.sh: 7: Syntax error: "(" unexpected

SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" > /dev/null 2>&1 && pwd )

SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"


=============================================================================
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
decayed_learning_rate_at_crossover_point = 1.500000e-03, adjusted_init_lr = 1.500000e-03
decayed_learning_rate_at_crossover_point = 1.500000e-03, adjusted_init_lr = 1.500000e-03
2023-02-07 02:27:43.042565: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-02-07 02:27:43.067403: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2100000000 Hz
2023-02-07 02:27:43.828685: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-02-07 02:27:43.842282: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2100000000 Hz
2023-02-07 02:27:44.502405: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff8d4001410 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-07 02:27:44.502482: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0
2023-02-07 02:27:44.502501: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100-PCIE-40GB, Compute Capability 8.0
2023-02-07 02:27:44.716619: I tensorflow/compiler/jit/xla_compilation_cache.cc:337] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2023-02-07 02:27:45.511987: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff8c0001410 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-07 02:27:45.512062: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0
2023-02-07 02:27:45.512082: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100-PCIE-40GB, Compute Capability 8.0
2023-02-07 02:27:45.720379: I tensorflow/compiler/jit/xla_compilation_cache.cc:337] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.







2023-02-07 02:28:54.522465: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-02-07 02:28:58.512659: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-02-07 02:29:02.694873: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-07 02:29:03.327776: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-07 02:29:07.203526: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-07 02:29:07.846401: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11

==========================================================================================
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
decayed_learning_rate_at_crossover_point = 3.000000e-03, adjusted_init_lr = 3.000000e-03
decayed_learning_rate_at_crossover_point = 3.000000e-03, adjusted_init_lr = 3.000000e-03
decayed_learning_rate_at_crossover_point = 3.000000e-03, adjusted_init_lr = 3.000000e-03
decayed_learning_rate_at_crossover_point = 3.000000e-03, adjusted_init_lr = 3.000000e-03
2023-02-07 02:22:05.880293: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-02-07 02:22:05.902731: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000000000 Hz
2023-02-07 02:22:05.997846: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-02-07 02:22:06.008864: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000000000 Hz
2023-02-07 02:22:06.094363: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-02-07 02:22:06.105314: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000000000 Hz
2023-02-07 02:22:06.633906: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-02-07 02:22:06.645009: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000000000 Hz
2023-02-07 02:22:08.735343: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff9c80015b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-07 02:22:08.735373: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-07 02:22:08.735378: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-07 02:22:08.735382: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-07 02:22:08.735386: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-07 02:22:08.860348: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff7d00015b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-07 02:22:08.860392: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-07 02:22:08.860398: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-07 02:22:08.860403: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-07 02:22:08.860409: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-07 02:22:08.883051: I tensorflow/compiler/jit/xla_compilation_cache.cc:337] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2023-02-07 02:22:08.923083: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff7ac0015b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-07 02:22:08.923120: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-07 02:22:08.923124: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-07 02:22:08.923127: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-07 02:22:08.923131: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-07 02:22:09.013114: I tensorflow/compiler/jit/xla_compilation_cache.cc:337] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2023-02-07 02:22:09.054342: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff7ec001410 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-07 02:22:09.054374: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-07 02:22:09.054379: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-07 02:22:09.054384: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-07 02:22:09.054388: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-07 02:22:09.055416: I tensorflow/compiler/jit/xla_compilation_cache.cc:337] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2023-02-07 02:22:09.185638: I tensorflow/compiler/jit/xla_compilation_cache.cc:337] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2023-02-07 02:23:06.418783: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-02-07 02:23:06.596981: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-02-07 02:23:06.598869: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-02-07 02:23:07.728627: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-02-07 02:23:12.983536: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-07 02:23:13.065151: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-07 02:23:13.381525: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-07 02:23:13.654418: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-07 02:23:13.731297: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-07 02:23:13.936548: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-07 02:23:14.578270: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-07 02:23:15.094386: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11

==================================================

오늘 함 달리까염
=====================================================
yop
TSBS (Time-series Benchmark Suite) is a benchmarking suite that allows testing of time-series databases. Time-series databases are databases that are optimized for handling time-series data, i.e. data that is sampled at regular intervals and stored over time. Some examples of time-series data are stock prices, weather data, server performance metrics, and IoT sensor data.

The goal of the TSBS benchmark suite is to evaluate the performance of different time-series databases and help users choose the right database for their needs. The suite consists of a set of tests and workloads that measure the performance of the database in terms of metrics such as insert rate, query rate, latency, and memory usage. These metrics are used to compare the performance of different databases and evaluate their strengths and weaknesses.

TSBS is written in Golang and uses a modular architecture, which allows users to easily extend and customize the benchmarking suite to fit their specific use case.


===============================
root@node001:~/bert_tf2# bash scripts/run_pretraining_lamb.sh 60 10 8 7.5e-4 5e-4 fp16 true 2 2000 200 7820 100 64 192 large
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
Container nvidia build =  19902717
Saving checkpoints to /results/tf_bert_pretraining_lamb_large_fp16_gbs17680_gbs23840_230203072318
Logs written to /results/tf_bert_pretraining_lamb_large_fp16_gbs17680_gbs23840_230203072318/tf_bert_pretraining_lamb_large_fp16_gbs17680_gbs23840.230203072318.log
+ bash scripts/run_pretraining_lamb_phase1.sh 60 10 8 7.500000e-04 5.000000e-04 fp16 true 2 2000 200 7820 100 64 192 large
+ tee -a /results/tf_bert_pretraining_lamb_large_fp16_gbs17680_gbs23840_230203072318/tf_bert_pretraining_lamb_large_fp16_gbs17680_gbs23840.230203072318.log
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
Container nvidia build =  19902717
XLA activated
2023-02-03 07:23:18.316172: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 07:23:18.316173: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:54: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported).
 The versions of TensorFlow you are currently using is 2.4.0 and is not supported.
Some things might work, some things might not.
If you were to encounter a bug, do not file an issue.
If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version.
You can find the compatibility matrix in TensorFlow Addon's readme:
https://github.com/tensorflow/addons
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:54: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported).
 The versions of TensorFlow you are currently using is 2.4.0 and is not supported.
Some things might work, some things might not.
If you were to encounter a bug, do not file an issue.
If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version.
You can find the compatibility matrix in TensorFlow Addon's readme:
https://github.com/tensorflow/addons
  warnings.warn(
2023-02-03 07:23:19.697999: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 07:23:19.698061: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 07:23:19.698660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2023-02-03 07:23:19.698694: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2023-02-03 07:23:20.058448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 0 with properties:
pciBusID: 0000:17:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 07:23:20.059980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 1 with properties:
pciBusID: 0000:65:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 07:23:20.060491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 0 with properties:
pciBusID: 0000:17:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 07:23:20.061408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 2 with properties:
pciBusID: 0000:ca:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 07:23:20.061991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 1 with properties:
pciBusID: 0000:65:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 07:23:20.062832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 3 with properties:
pciBusID: 0000:e3:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 07:23:20.062846: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 07:23:20.063343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 2 with properties:
pciBusID: 0000:ca:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 07:23:20.064502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 3 with properties:
pciBusID: 0000:e3:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 07:23:20.064514: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 07:23:20.065734: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 07:23:20.065774: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 07:23:20.066493: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-02-03 07:23:20.066673: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-02-03 07:23:20.067192: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 07:23:20.067225: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 07:23:20.067887: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-02-03 07:23:20.068059: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-02-03 07:23:20.069504: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2023-02-03 07:23:20.070202: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-02-03 07:23:20.070301: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-02-03 07:23:20.070749: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2023-02-03 07:23:20.071428: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-02-03 07:23:20.071525: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-02-03 07:23:20.080257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1888] Adding visible gpu devices: 0, 1, 2, 3
2023-02-03 07:23:20.081500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1888] Adding visible gpu devices: 0, 1, 2, 3
2023-02-03 07:23:20.228248: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 07:23:20.229629: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 07:23:20.230933: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 07:23:20.232094: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
I0203 07:23:20.233313 140737349982016 device_compatibility_check.py:126] Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
W0203 07:23:20.233552 140737349982016 deprecation.py:333] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
2023-02-03 07:23:20.238903: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 07:23:20.240155: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 07:23:20.241346: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 07:23:20.242506: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
I0203 07:23:20.243711 140737349982016 device_compatibility_check.py:126] Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
W0203 07:23:20.243941 140737349982016 deprecation.py:333] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
2023-02-03 07:23:20.245077: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 07:23:20.246344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 0 with properties:
pciBusID: 0000:65:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 07:23:20.246362: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 07:23:20.246385: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 07:23:20.246394: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 07:23:20.246403: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-02-03 07:23:20.246411: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-02-03 07:23:20.246420: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2023-02-03 07:23:20.246428: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-02-03 07:23:20.246436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-02-03 07:23:20.248737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1888] Adding visible gpu devices: 1
2023-02-03 07:23:20.248756: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 07:23:20.254763: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 07:23:20.264310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 0 with properties:
pciBusID: 0000:17:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 07:23:20.264334: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 07:23:20.264349: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 07:23:20.264356: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 07:23:20.264362: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-02-03 07:23:20.264368: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-02-03 07:23:20.264374: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2023-02-03 07:23:20.264380: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-02-03 07:23:20.264386: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-02-03 07:23:20.299170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1888] Adding visible gpu devices: 0
2023-02-03 07:23:20.299211: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 07:23:20.833752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1287] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-02-03 07:23:20.833790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1293]      1
2023-02-03 07:23:20.833796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306] 1:   N
2023-02-03 07:23:20.836310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 78515 MB memory) -> physical GPU (device: 1, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:65:00.0, compute capability: 8.0)
2023-02-03 07:23:20.854678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1287] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-02-03 07:23:20.854721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1293]      0
2023-02-03 07:23:20.854727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306] 0:   N
2023-02-03 07:23:20.859386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 78515 MB memory) -> physical GPU (device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:17:00.0, compute capability: 8.0)
I0203 07:23:20.870215 140737349982016 run_pretraining.py:123] init_lr = 0.001500
I0203 07:23:20.893343 140737349982016 run_pretraining.py:123] init_lr = 0.001500
WARNING:tensorflow:
The following Variables were used a Lambda layer's call (lambda_2), but
are not present in its tracked objects:
  <tf.Variable 'word_embeddings/embeddings:0' shape=(30522, 1024) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
W0203 07:23:24.027718 140737349982016 core.py:966]
The following Variables were used a Lambda layer's call (lambda_2), but
are not present in its tracked objects:
  <tf.Variable 'word_embeddings/embeddings:0' shape=(30522, 1024) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
WARNING:tensorflow:
The following Variables were used a Lambda layer's call (lambda_2), but
are not present in its tracked objects:
  <tf.Variable 'word_embeddings/embeddings:0' shape=(30522, 1024) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
W0203 07:23:24.071393 140737349982016 core.py:966]
The following Variables were used a Lambda layer's call (lambda_2), but
are not present in its tracked objects:
  <tf.Variable 'word_embeddings/embeddings:0' shape=(30522, 1024) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
decayed_learning_rate_at_crossover_point = 1.500000e-03, adjusted_init_lr = 1.500000e-03
decayed_learning_rate_at_crossover_point = 1.500000e-03, adjusted_init_lr = 1.500000e-03
2023-02-03 07:23:49.647813: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-02-03 07:23:49.670160: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000000000 Hz
2023-02-03 07:23:50.136915: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-02-03 07:23:50.148164: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000000000 Hz
2023-02-03 07:23:51.197809: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ffa800015b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-03 07:23:51.197841: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 07:23:51.197846: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 07:23:51.197850: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 07:23:51.197853: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 07:23:51.358562: I tensorflow/compiler/jit/xla_compilation_cache.cc:337] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2023-02-03 07:23:51.804948: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff7680015b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-03 07:23:51.804983: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 07:23:51.804989: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 07:23:51.804993: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 07:23:51.804997: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 07:23:51.956817: I tensorflow/compiler/jit/xla_compilation_cache.cc:337] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2023-02-03 07:24:49.967486: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-02-03 07:24:51.576128: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-02-03 07:24:56.431147: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 07:24:56.930047: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 07:24:58.439141: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 07:24:58.945183: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11



================================================================
root@node001:~/bert_tf2# bash scripts/run_pretraining_lamb.sh 60 10 8 7.5e-4 5e-4 fp16 true 4 2000 200 7820 100 64 192 large
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
Container nvidia build =  19902717
Saving checkpoints to /results/tf_bert_pretraining_lamb_large_fp16_gbs115360_gbs27680_230203052322
Logs written to /results/tf_bert_pretraining_lamb_large_fp16_gbs115360_gbs27680_230203052322/tf_bert_pretraining_lamb_large_fp16_gbs115360_gbs27680.230203052322.log
+ bash scripts/run_pretraining_lamb_phase1.sh 60 10 8 7.500000e-04 5.000000e-04 fp16 true 4 2000 200 7820 100 64 192 large
+ tee -a /results/tf_bert_pretraining_lamb_large_fp16_gbs115360_gbs27680_230203052322/tf_bert_pretraining_lamb_large_fp16_gbs115360_gbs27680.230203052322.log
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
Container nvidia build =  19902717
XLA activated
2023-02-03 05:23:22.598541: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:23:22.598541: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:23:22.598543: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:23:22.598542: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:54: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported).
 The versions of TensorFlow you are currently using is 2.4.0 and is not supported.
Some things might work, some things might not.
If you were to encounter a bug, do not file an issue.
If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version.
You can find the compatibility matrix in TensorFlow Addon's readme:
https://github.com/tensorflow/addons
  warnings.warn(
2023-02-03 05:23:23.617018: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:23.617666: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:54: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported).
 The versions of TensorFlow you are currently using is 2.4.0 and is not supported.
Some things might work, some things might not.
If you were to encounter a bug, do not file an issue.
If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version.
You can find the compatibility matrix in TensorFlow Addon's readme:
https://github.com/tensorflow/addons
  warnings.warn(
2023-02-03 05:23:23.625697: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:23.626244: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:54: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported).
 The versions of TensorFlow you are currently using is 2.4.0 and is not supported.
Some things might work, some things might not.
If you were to encounter a bug, do not file an issue.
If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version.
You can find the compatibility matrix in TensorFlow Addon's readme:
https://github.com/tensorflow/addons
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:54: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported).
 The versions of TensorFlow you are currently using is 2.4.0 and is not supported.
Some things might work, some things might not.
If you were to encounter a bug, do not file an issue.
If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version.
You can find the compatibility matrix in TensorFlow Addon's readme:
https://github.com/tensorflow/addons
  warnings.warn(
2023-02-03 05:23:24.341274: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:24.341293: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:24.341937: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2023-02-03 05:23:24.341944: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2023-02-03 05:23:24.684496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 0 with properties:
pciBusID: 0000:17:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:24.687408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 1 with properties:
pciBusID: 0000:65:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:24.689699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 2 with properties:
pciBusID: 0000:ca:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:24.690663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 0 with properties:
pciBusID: 0000:17:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:24.692511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 3 with properties:
pciBusID: 0000:e3:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:24.692526: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:23:24.694186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 1 with properties:
pciBusID: 0000:65:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:24.695428: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 05:23:24.695471: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 05:23:24.696163: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-02-03 05:23:24.696349: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-02-03 05:23:24.697674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 2 with properties:
pciBusID: 0000:ca:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:24.699142: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2023-02-03 05:23:24.699743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 0 with properties:
pciBusID: 0000:17:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:24.699870: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-02-03 05:23:24.699970: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-02-03 05:23:24.700067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 3 with properties:
pciBusID: 0000:e3:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:24.700081: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:23:24.702141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 1 with properties:
pciBusID: 0000:65:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:24.702680: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 05:23:24.702727: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 05:23:24.703291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 2 with properties:
pciBusID: 0000:ca:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:24.703396: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-02-03 05:23:24.703570: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-02-03 05:23:24.704965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 3 with properties:
pciBusID: 0000:e3:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:24.704977: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:23:24.705897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 0 with properties:
pciBusID: 0000:17:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:24.706209: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2023-02-03 05:23:24.706879: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-02-03 05:23:24.706974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-02-03 05:23:24.707305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 1 with properties:
pciBusID: 0000:65:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:24.707618: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 05:23:24.707655: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 05:23:24.708299: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-02-03 05:23:24.708465: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-02-03 05:23:24.708564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 2 with properties:
pciBusID: 0000:ca:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:24.709863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 3 with properties:
pciBusID: 0000:e3:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:24.709877: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:23:24.711034: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2023-02-03 05:23:24.711456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1888] Adding visible gpu devices: 0, 1, 2, 3
2023-02-03 05:23:24.711695: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-02-03 05:23:24.711787: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-02-03 05:23:24.712543: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 05:23:24.712576: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 05:23:24.713264: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-02-03 05:23:24.713435: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-02-03 05:23:24.716111: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2023-02-03 05:23:24.716787: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-02-03 05:23:24.716881: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-02-03 05:23:24.717855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1888] Adding visible gpu devices: 0, 1, 2, 3
2023-02-03 05:23:24.725760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1888] Adding visible gpu devices: 0, 1, 2, 3
2023-02-03 05:23:24.729105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1888] Adding visible gpu devices: 0, 1, 2, 3
2023-02-03 05:23:25.019443: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:25.020391: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:25.021800: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:25.022737: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:25.022902: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:25.023814: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
I0203 05:23:25.023981 140737349982016 device_compatibility_check.py:126] Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
W0203 05:23:25.024234 140737349982016 deprecation.py:333] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
2023-02-03 05:23:25.025061: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:25.025896: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
I0203 05:23:25.026960 140737349982016 device_compatibility_check.py:126] Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
W0203 05:23:25.027184 140737349982016 deprecation.py:333] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
2023-02-03 05:23:25.032209: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:25.033111: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:25.034312: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:25.035124: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
I0203 05:23:25.036181 140737349982016 device_compatibility_check.py:126] Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
W0203 05:23:25.036411 140737349982016 deprecation.py:333] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
2023-02-03 05:23:25.036920: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:25.037379: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:25.038057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 0 with properties:
pciBusID: 0000:e3:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:25.038083: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:23:25.038147: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 05:23:25.038158: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 05:23:25.038167: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-02-03 05:23:25.038177: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-02-03 05:23:25.038186: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2023-02-03 05:23:25.038195: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-02-03 05:23:25.038205: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-02-03 05:23:25.038336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 0 with properties:
pciBusID: 0000:17:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:25.038356: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:23:25.038378: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 05:23:25.038387: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 05:23:25.038396: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-02-03 05:23:25.038404: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-02-03 05:23:25.038412: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2023-02-03 05:23:25.038420: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-02-03 05:23:25.038430: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-02-03 05:23:25.040164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1888] Adding visible gpu devices: 0
2023-02-03 05:23:25.040190: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:23:25.040540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1888] Adding visible gpu devices: 3
2023-02-03 05:23:25.040573: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:23:25.042100: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:25.044263: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:25.047394: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:25.047574: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:25.052760: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:25.054494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 0 with properties:
pciBusID: 0000:65:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:25.054522: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:23:25.054546: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 05:23:25.054554: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 05:23:25.054562: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-02-03 05:23:25.054569: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-02-03 05:23:25.054576: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2023-02-03 05:23:25.054584: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-02-03 05:23:25.054591: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
I0203 05:23:25.059841 140737349982016 device_compatibility_check.py:126] Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
W0203 05:23:25.060071 140737349982016 deprecation.py:333] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
2023-02-03 05:23:25.063757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1888] Adding visible gpu devices: 1
2023-02-03 05:23:25.063788: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:23:25.070483: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:23:25.093413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 0 with properties:
pciBusID: 0000:ca:00.0 name: NVIDIA A100 80GB PCIe computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 79.21GiB deviceMemoryBandwidth: 1.76TiB/s
2023-02-03 05:23:25.093441: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:23:25.093461: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 05:23:25.093467: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 05:23:25.093473: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-02-03 05:23:25.093479: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-02-03 05:23:25.093485: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2023-02-03 05:23:25.093491: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-02-03 05:23:25.093499: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-02-03 05:23:25.127034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1888] Adding visible gpu devices: 2
2023-02-03 05:23:25.127070: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:23:26.543144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1287] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-02-03 05:23:26.543197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1293]      0
2023-02-03 05:23:26.543205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306] 0:   N
2023-02-03 05:23:26.546668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 78515 MB memory) -> physical GPU (device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:17:00.0, compute capability: 8.0)
2023-02-03 05:23:26.559279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1287] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-02-03 05:23:26.559315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1293]      1
2023-02-03 05:23:26.559321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306] 1:   N
2023-02-03 05:23:26.562975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 78515 MB memory) -> physical GPU (device: 1, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:65:00.0, compute capability: 8.0)
I0203 05:23:26.591860 140737349982016 run_pretraining.py:123] init_lr = 0.003000
I0203 05:23:26.607615 140737349982016 run_pretraining.py:123] init_lr = 0.003000
2023-02-03 05:23:26.627045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1287] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-02-03 05:23:26.627084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1293]      3
2023-02-03 05:23:26.627094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306] 3:   N
2023-02-03 05:23:26.629675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 78515 MB memory) -> physical GPU (device: 3, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:e3:00.0, compute capability: 8.0)
2023-02-03 05:23:26.649690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1287] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-02-03 05:23:26.649719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1293]      2
2023-02-03 05:23:26.649724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306] 2:   N
2023-02-03 05:23:26.653091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 78515 MB memory) -> physical GPU (device: 2, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:ca:00.0, compute capability: 8.0)
I0203 05:23:26.667915 140737349982016 run_pretraining.py:123] init_lr = 0.003000
I0203 05:23:26.710142 140737349982016 run_pretraining.py:123] init_lr = 0.003000
WARNING:tensorflow:
The following Variables were used a Lambda layer's call (lambda_2), but
are not present in its tracked objects:
  <tf.Variable 'word_embeddings/embeddings:0' shape=(30522, 1024) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
W0203 05:23:29.852304 140737349982016 core.py:966]
The following Variables were used a Lambda layer's call (lambda_2), but
are not present in its tracked objects:
  <tf.Variable 'word_embeddings/embeddings:0' shape=(30522, 1024) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
WARNING:tensorflow:
The following Variables were used a Lambda layer's call (lambda_2), but
are not present in its tracked objects:
  <tf.Variable 'word_embeddings/embeddings:0' shape=(30522, 1024) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
W0203 05:23:29.872107 140737349982016 core.py:966]
The following Variables were used a Lambda layer's call (lambda_2), but
are not present in its tracked objects:
  <tf.Variable 'word_embeddings/embeddings:0' shape=(30522, 1024) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
WARNING:tensorflow:
The following Variables were used a Lambda layer's call (lambda_2), but
are not present in its tracked objects:
  <tf.Variable 'word_embeddings/embeddings:0' shape=(30522, 1024) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
W0203 05:23:29.896322 140737349982016 core.py:966]
The following Variables were used a Lambda layer's call (lambda_2), but
are not present in its tracked objects:
  <tf.Variable 'word_embeddings/embeddings:0' shape=(30522, 1024) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
WARNING:tensorflow:
The following Variables were used a Lambda layer's call (lambda_2), but
are not present in its tracked objects:
  <tf.Variable 'word_embeddings/embeddings:0' shape=(30522, 1024) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
W0203 05:23:29.917098 140737349982016 core.py:966]
The following Variables were used a Lambda layer's call (lambda_2), but
are not present in its tracked objects:
  <tf.Variable 'word_embeddings/embeddings:0' shape=(30522, 1024) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
decayed_learning_rate_at_crossover_point = 3.000000e-03, adjusted_init_lr = 3.000000e-03
decayed_learning_rate_at_crossover_point = 3.000000e-03, adjusted_init_lr = 3.000000e-03
decayed_learning_rate_at_crossover_point = 3.000000e-03, adjusted_init_lr = 3.000000e-03
decayed_learning_rate_at_crossover_point = 3.000000e-03, adjusted_init_lr = 3.000000e-03
2023-02-03 05:23:55.238692: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-02-03 05:23:55.261170: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000000000 Hz
2023-02-03 05:23:55.298772: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-02-03 05:23:55.309888: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000000000 Hz
2023-02-03 05:23:55.511093: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-02-03 05:23:55.534182: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000000000 Hz
2023-02-03 05:23:56.063608: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-02-03 05:23:56.074877: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000000000 Hz
2023-02-03 05:23:59.227456: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff78c0015b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-03 05:23:59.227491: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 05:23:59.227497: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 05:23:59.227500: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 05:23:59.227504: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 05:23:59.343695: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff7b80015b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-03 05:23:59.343733: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 05:23:59.343738: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 05:23:59.343741: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 05:23:59.343746: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 05:23:59.373771: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff7dc0015b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-03 05:23:59.373797: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 05:23:59.373801: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 05:23:59.373805: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 05:23:59.373808: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 05:23:59.394562: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff7d00015b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-03 05:23:59.394597: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 05:23:59.394601: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 05:23:59.394605: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 05:23:59.394608: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): NVIDIA A100 80GB PCIe, Compute Capability 8.0
2023-02-03 05:23:59.395932: I tensorflow/compiler/jit/xla_compilation_cache.cc:337] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2023-02-03 05:23:59.490859: I tensorflow/compiler/jit/xla_compilation_cache.cc:337] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2023-02-03 05:23:59.514782: I tensorflow/compiler/jit/xla_compilation_cache.cc:337] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2023-02-03 05:23:59.553320: I tensorflow/compiler/jit/xla_compilation_cache.cc:337] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2023-02-03 05:24:56.977716: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-02-03 05:24:57.193794: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-02-03 05:24:58.198169: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-02-03 05:24:58.829898: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-02-03 05:25:03.305933: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 05:25:03.647676: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 05:25:03.808190: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11

2023-02-03 05:25:04.226548: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11

2023-02-03 05:25:04.905726: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 05:25:05.546761: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 05:25:05.687551: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 05:25:06.236576: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11


=============================================================
root@node002:~/bert_tf2# bash scripts/run_pretraining_lamb.sh 60 10 8 7.5e-4 5e-4 fp16 true 2 2000 200 7820 100 64 192 large
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
Container nvidia build =  19902717
Saving checkpoints to /results/tf_bert_pretraining_lamb_large_fp16_gbs17680_gbs23840_230203055722
Logs written to /results/tf_bert_pretraining_lamb_large_fp16_gbs17680_gbs23840_230203055722/tf_bert_pretraining_lamb_large_fp16_gbs17680_gbs23840.230203055722.log
+ bash scripts/run_pretraining_lamb_phase1.sh 60 10 8 7.500000e-04 5.000000e-04 fp16 true 2 2000 200 7820 100 64 192 large
+ tee -a /results/tf_bert_pretraining_lamb_large_fp16_gbs17680_gbs23840_230203055722/tf_bert_pretraining_lamb_large_fp16_gbs17680_gbs23840.230203055722.log
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
Container nvidia build =  19902717
XLA activated
2023-02-03 05:57:22.750717: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:57:22.750717: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:54: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported).
 The versions of TensorFlow you are currently using is 2.4.0 and is not supported.
Some things might work, some things might not.
If you were to encounter a bug, do not file an issue.
If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version.
You can find the compatibility matrix in TensorFlow Addon's readme:
https://github.com/tensorflow/addons
  warnings.warn(
2023-02-03 05:57:24.271637: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:54: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported).
 The versions of TensorFlow you are currently using is 2.4.0 and is not supported.
Some things might work, some things might not.
If you were to encounter a bug, do not file an issue.
If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version.
You can find the compatibility matrix in TensorFlow Addon's readme:
https://github.com/tensorflow/addons
  warnings.warn(
2023-02-03 05:57:24.272523: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:57:24.272878: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2023-02-03 05:57:24.273830: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2023-02-03 05:57:24.522574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 0 with properties:
pciBusID: 0000:af:00.0 name: NVIDIA A100-PCIE-40GB computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
2023-02-03 05:57:24.525978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 1 with properties:
pciBusID: 0000:d8:00.0 name: NVIDIA A100-PCIE-40GB computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
2023-02-03 05:57:24.526004: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:57:24.528315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 0 with properties:
pciBusID: 0000:af:00.0 name: NVIDIA A100-PCIE-40GB computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
2023-02-03 05:57:24.530068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 1 with properties:
pciBusID: 0000:d8:00.0 name: NVIDIA A100-PCIE-40GB computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
2023-02-03 05:57:24.530109: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:57:24.531024: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 05:57:24.531097: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 05:57:24.532429: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-02-03 05:57:24.532792: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-02-03 05:57:24.536585: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 05:57:24.536614: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2023-02-03 05:57:24.536645: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 05:57:24.537630: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-02-03 05:57:24.537767: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-02-03 05:57:24.537927: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-02-03 05:57:24.538275: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-02-03 05:57:24.543475: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2023-02-03 05:57:24.544002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1888] Adding visible gpu devices: 0, 1
2023-02-03 05:57:24.544653: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-02-03 05:57:24.544783: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-02-03 05:57:24.551055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1888] Adding visible gpu devices: 0, 1
2023-02-03 05:57:24.768149: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:57:24.770066: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
2023-02-03 05:57:24.772051: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
I0203 05:57:24.771905 140737349957440 device_compatibility_check.py:126] Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
W0203 05:57:24.772253 140737349957440 deprecation.py:333] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
2023-02-03 05:57:24.773772: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
I0203 05:57:24.775626 140737349957440 device_compatibility_check.py:126] Mixed precision compatibility check (mixed_float16): OK
Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0
WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
W0203 05:57:24.775983 140737349957440 deprecation.py:333] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale
2023-02-03 05:57:24.800199: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:57:24.800293: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2023-02-03 05:57:24.802062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 0 with properties:
pciBusID: 0000:af:00.0 name: NVIDIA A100-PCIE-40GB computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
2023-02-03 05:57:24.802095: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:57:24.802132: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 05:57:24.802144: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 05:57:24.802156: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-02-03 05:57:24.802168: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-02-03 05:57:24.802181: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2023-02-03 05:57:24.802192: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-02-03 05:57:24.802205: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-02-03 05:57:24.802201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Found device 0 with properties:
pciBusID: 0000:d8:00.0 name: NVIDIA A100-PCIE-40GB computeCapability: 8.0
coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.45GiB deviceMemoryBandwidth: 1.41TiB/s
2023-02-03 05:57:24.802223: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:57:24.802257: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 05:57:24.802280: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 05:57:24.802294: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2023-02-03 05:57:24.802306: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2023-02-03 05:57:24.802319: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.11
2023-02-03 05:57:24.802331: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2023-02-03 05:57:24.802343: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2023-02-03 05:57:24.805686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1888] Adding visible gpu devices: 0
2023-02-03 05:57:24.805729: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:57:24.805831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1888] Adding visible gpu devices: 1
2023-02-03 05:57:24.805859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2023-02-03 05:57:25.641719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1287] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-02-03 05:57:25.641765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1293]      0
2023-02-03 05:57:25.641772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306] 0:   N
2023-02-03 05:57:25.646476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 37800 MB memory) -> physical GPU (device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:af:00.0, compute capability: 8.0)
2023-02-03 05:57:25.652653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1287] Device interconnect StreamExecutor with strength 1 edge matrix:
2023-02-03 05:57:25.652700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1293]      1
2023-02-03 05:57:25.652707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306] 1:   N
2023-02-03 05:57:25.657622: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 37800 MB memory) -> physical GPU (device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:d8:00.0, compute capability: 8.0)
I0203 05:57:25.676026 140737349957440 run_pretraining.py:123] init_lr = 0.001500
I0203 05:57:25.682729 140737349957440 run_pretraining.py:123] init_lr = 0.001500
WARNING:tensorflow:
The following Variables were used a Lambda layer's call (lambda_2), but
are not present in its tracked objects:
  <tf.Variable 'word_embeddings/embeddings:0' shape=(30522, 1024) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
W0203 05:57:29.200984 140737349957440 core.py:966]
The following Variables were used a Lambda layer's call (lambda_2), but
are not present in its tracked objects:
  <tf.Variable 'word_embeddings/embeddings:0' shape=(30522, 1024) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
WARNING:tensorflow:
The following Variables were used a Lambda layer's call (lambda_2), but
are not present in its tracked objects:
  <tf.Variable 'word_embeddings/embeddings:0' shape=(30522, 1024) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
W0203 05:57:29.272504 140737349957440 core.py:966]
The following Variables were used a Lambda layer's call (lambda_2), but
are not present in its tracked objects:
  <tf.Variable 'word_embeddings/embeddings:0' shape=(30522, 1024) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
decayed_learning_rate_at_crossover_point = 1.500000e-03, adjusted_init_lr = 1.500000e-03
decayed_learning_rate_at_crossover_point = 1.500000e-03, adjusted_init_lr = 1.500000e-03





2023-02-03 05:57:58.196798: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-02-03 05:57:58.211253: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2100000000 Hz
2023-02-03 05:57:59.110056: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2023-02-03 05:57:59.124726: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2100000000 Hz
2023-02-03 05:57:59.764341: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff904001410 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-03 05:57:59.764417: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0
2023-02-03 05:57:59.764442: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100-PCIE-40GB, Compute Capability 8.0
2023-02-03 05:57:59.999380: I tensorflow/compiler/jit/xla_compilation_cache.cc:337] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2023-02-03 05:58:00.720522: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff8b4001410 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-02-03 05:58:00.720594: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0
2023-02-03 05:58:00.720622: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA A100-PCIE-40GB, Compute Capability 8.0
2023-02-03 05:58:00.937379: I tensorflow/compiler/jit/xla_compilation_cache.cc:337] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.




2023-02-03 05:59:08.579223: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-02-03 05:59:12.747219: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-02-03 05:59:16.766041: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 05:59:17.414011: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11


2023-02-03 05:59:21.527002: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11


2023-02-03 05:59:22.167250: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11


I0203 06:22:52.553852 140737349957440 model_training_utils.py:523] Saved checkpoint to /results/tf_bert_pretraining_lamb_large_fp16_gbs17680_gbs23840_230203055722/phase_1/ckpt-1
I0203 06:22:52.563970 140737349957440 model_training_utils.py:538] Step: 100 Lr 7.575e-05 Loss scale 32768
I0203 06:22:52.564061 140737349957440 model_training_utils.py:539] Train Step: 100/7038  / loss = 10.353649139404297 / time = 1516.255 sec  masked_lm_accuracy = 0.036602  lm_example_loss = 16.000000  next_sentence_accuracy = 0.551070  next_sentence_loss = 1.000000
I0203 06:22:52.564102 140737349957440 model_training_utils.py:540] Perf 506.51
I0203 06:47:43.378864 140737349957440 model_training_utils.py:523] Saved checkpoint to /results/tf_bert_pretraining_lamb_large_fp16_gbs17680_gbs23840_230203055722/phase_1/ckpt-2
I0203 06:47:43.393725 140737349957440 model_training_utils.py:538] Step: 200 Lr 0.0001515 Loss scale 32768
I0203 06:47:43.393826 140737349957440 model_training_utils.py:539] Train Step: 200/7038  / loss = 9.26272964477539 / time = 1485.074 sec  masked_lm_accuracy = 0.070117  lm_example_loss = 16.000000  next_sentence_accuracy = 0.608391  next_sentence_loss = 1.000000
I0203 06:47:43.393873 140737349957440 model_training_utils.py:540] Perf 517.15



===========================
2023-02-03 04:58:26.700652: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
Traceback (most recent call last):
  File "/workspace/bert_tf2/run_pretraining.py", line 207, in <module>
    app.run(main)
  File "/usr/local/lib/python3.8/dist-packages/absl/app.py", line 300, in run
    _run_main(main, args)
  File "/usr/local/lib/python3.8/dist-packages/absl/app.py", line 251, in _run_main
    sys.exit(main(argv))
  File "/workspace/bert_tf2/run_pretraining.py", line 203, in main
    run_bert_pretrain(strategy)
  File "/workspace/bert_tf2/run_pretraining.py", line 157, in run_bert_pretrain
    return run_customized_training(
  File "/workspace/bert_tf2/run_pretraining.py", line 124, in run_customized_training
    trained_model = model_training_utils.run_customized_training_loop(
  File "/workspace/bert_tf2/official/modeling/model_training_utils.py", line 506, in run_customized_training_loop
    train_steps(train_iterator,
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 828, in __call__
    result = self._call(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py", line 888, in _call
    return self._stateless_fn(*args, **kwds)
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 2942, in __call__
    return graph_function._call_flat(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 1918, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py", line 555, in call
    outputs = execute.execute(
  File "/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.CancelledError:  [_Derived_]RecvAsync is cancelled.
         [[{{node cluster_13_1/merge_oidx_0/_60}}]] [Op:__inference_train_steps_132767]

Function call stack:
train_steps


===================================================================
2023-02-03 03:01:06.704637: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-02-03 03:01:07.516794: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-02-03 03:01:08.473505: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-02-03 03:01:08.722048: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1642] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2023-02-03 03:01:13.241155: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 03:01:14.217943: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 03:01:14.499781: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 03:01:14.949386: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 03:01:15.070425: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 03:01:15.653060: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2023-02-03 03:01:15.981874: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2023-02-03 03:01:16.816955: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11



======================================================
I0202 05:01:13.666598 140737349957440 model_training_utils.py:523] Saved checkpoint to /results/tf_bert_pretraining_lamb_large_fp16_gbs17680_gbs23840_230202043547/phase_1/ckpt-1
I0202 05:01:13.687793 140737349957440 model_training_utils.py:538] Step: 100 Lr 7.575e-05 Loss scale 32768
I0202 05:01:13.687882 140737349957440 model_training_utils.py:539] Train Step: 100/7038  / loss = 10.430100440979004 / time = 1511.716 sec  masked_lm_accuracy = 0.037829  lm_example_loss = 16.000000  next_sentence_accuracy = 0.543987  next_sentence_loss = 1.000000
I0202 05:01:13.687921 140737349957440 model_training_utils.py:540] Perf 508.03




I0202 05:25:58.336649 140737349957440 model_training_utils.py:523] Saved checkpoint to /results/tf_bert_pretraining_lamb_large_fp16_gbs17680_gbs23840_230202043547/phase_1/ckpt-2
I0202 05:25:58.344990 140737349957440 model_training_utils.py:538] Step: 200 Lr 0.0001515 Loss scale 32768
I0202 05:25:58.345090 140737349957440 model_training_utils.py:539] Train Step: 200/7038  / loss = 9.3572998046875 / time = 1479.187 sec  masked_lm_accuracy = 0.068735  lm_example_loss = 16.000000  next_sentence_accuracy = 0.603206  next_sentence_loss = 1.000000
I0202 05:25:58.345143 140737349957440 model_training_utils.py:540] Perf 519.21




================================================
Let me break down each step of the data preprocessing implemented by Nvidia's Deep Learning examples Tensorflow BERT:

Tokenization: This step involves breaking down the input text into individual tokens, where each token represents a word or punctuation. The tokenization is performed using the tokenizer provided by the BERT library, which uses a vocabulary file containing the most common words and subwords in the training data.

Input Encoding: After tokenization, each token is then assigned a unique numerical representation, called an ID. This encoding is used to feed the input data into the deep learning model. BERT uses a specific input encoding scheme, known as the BERT input encoding, which includes additional information such as the position of the tokens within the sequence and the sentence segment information.

Padding: To ensure that all input sequences have the same length, the shorter sequences are padded with zeros. The padding is done to the right side of the sequence.

Masking: BERT uses a technique called masking, where some of the tokens in the input sequence are masked, or replaced with a special token [MASK]. The purpose of masking is to predict the original token from the masked token, so the model learns to understand the context of the words in the sequence.

Batch preparation: The input data is then divided into smaller batches, where each batch contains a certain number of samples. The number of samples in a batch is a hyperparameter that can be adjusted based on the available memory and computational resources. The batches are then shuffled to ensure that the model sees a diverse set of input sequences during each training iteration.

These are the main steps involved in the data preprocessing in Nvidia's Deep Learning examples Tensorflow BERT.





Nvidia's Deep Learning examples for TensorFlow BERT implement data preprocessing using the following steps:

Tokenization: The raw text data is tokenized into subwords using WordPiece tokenization.

Mapping tokens to IDs: Each token is then mapped to a unique integer ID.

Padding: The token IDs are then padded to a maximum sequence length to ensure that all input sequences have the same length.

Creating attention masks: An attention mask is created to indicate which tokens in the input sequence should be attended to by the BERT model and which should be ignored.

Generating input tensors: The token IDs and attention masks are then converted into tensors, which are the input to the BERT model.

These steps are performed using the tokenizer and prepare_inputs functions provided by the TensorFlow BERT library. The preprocessed data can then be fed into the BERT model for training or fine-tuning.

============================================================
/usr/bin/python: Error while finding module specification for 'wikiextractor.WikiExtractor' (ModuleNotFoundError: No module named 'wikiextractor')


================================================
BookCorpus: A large-scale dataset of 11,038 books with over 1.4 million unique words. You can download the dataset from the following link: http://yknzhu.wixsite.com/mbweb

Wikipedia English: A large-scale dump of English Wikipedia that can be used to pre-train models. You can download the dataset from the following link: https://dumps.wikimedia.org/enwiki/

SQuAD 2.0: The Stanford Question Answering Dataset version 2.0 which consists of over 130,000 questions and answers. You can download the dataset from the following link: https://rajpurkar.github.io/SQuAD-explorer/

=================================================
run test-photo.py --help
usage: test-photo.py [-h] [-f FOLDER] [-n NUM_IMAGES]

Process multiple images using Photoshop

optional arguments:
  -h, --help            show this help message and exit
  -f FOLDER, --folder FOLDER
                        Folder path where images are located
  -n NUM_IMAGES, --num_images NUM_IMAGES
                        Number of images to process at once





https://docs.google.com/document/d/1Y2irk__m3ZCsDz1NVVhITwj9r8WXUg3jTUVrJxkdNQE/edit#


Chat GPT

Additional preprocessing steps, such as tokenization and batching, are important for preparing the data for training a model like ChatGPT. These steps are typically performed using the map and shuffle functions in TensorFlow 2 and PyTorch.

Tokenization: The process of breaking down the text into individual tokens, such as words or subwords. This is important because the model expects the input to be in the form of tokens, not raw text. Tokenization can be done using various techniques such as splitting the text by whitespaces or using a pre-trained tokenizer.

Batching: The process of grouping the data into smaller chunks or batches. This is important because the model is typically trained on a batch of data at a time, rather than on the entire dataset. Batching allows the model to process multiple examples at once, which can speed up the training process.

Shuffling: The process of randomly shuffling the data before each training epoch. This is important because it helps prevent the model from overfitting to the order of the data. By shuffling the data, each training epoch will be exposed to a different order of examples, which can help the model generalize better.

It's important to note that the tokenization, batching, and shuffling steps can be performed using different functions and libraries depending on the framework you use. The above script is just an example and it can be modified to use different tokenization method or different batch size.




The English Wikipedia dump is a large dataset containing the entire English Wikipedia in an XML format. The dump is released regularly and can be downloaded from the following website:

https://dumps.wikimedia.org/enwiki/

You will have to select the most recent available dump and download the corresponding file. The dump is usually compressed using bz2 or 7z format, you will have to decompress the file to access the data.

It's important to note that the file size can reach several gigabytes, so it may take some time to download the data. Also, the data is in XML format, so you may need to parse the data and perform some preprocessing before using it to train your model.




Generating 13GB/s IOs on storage media and DRAM for a ChatGPT model would be a very demanding task and would likely require a large and complex dataset as well as a powerful machine with a lot of memory and storage resources.

For the dataset, you can use a large corpus of text data such as books, articles or web pages. For example, you can use the English Wikipedia dump that contains several gigabytes of text data.

For the model, you can use a large pre-trained model such as GPT-3 or a similar transformer-based model. These models typically have billions of parameters and require a significant amount of memory and storage.

During training, the data is loaded into the DRAM in batches, where the model processes it to update the model's parameters. The data is also stored on the storage media, such as NVMe SSDs, and is read and written to frequently during the training process.

It's important to note that generating 13GB/s IOs is a very demanding task, and even with a large dataset and a powerful model, it may not be possible to achieve this level of performance on a single machine. You may need to use a cluster of machines or cloud resources to generate this level of IOs.

It's also important to note that generating 13GB/s IOs is a very demanding task and requires a large amount of memory and storage resources, as well as a powerful GPU to process the data quickly enough. It's not a common requirement for most use cases and it's important to consider the cost and feasibility before attempting this task.




=====
mount -t lustre 192.168.61.80@tcp:/lustre /mnt/lustre
======


import argparse
import subprocess
import os, fnmatch, shutil
import re
import parser
from random import seed
from random import randint
from pathlib import Path
from datetime import datetime
from paramiko import client
from distutils.dir_util import copy_tree
from scp import SCPClient, SCPException
import json
from time import sleep
from lib.remote import *
import lib.application as app
import apps.imagenet.imagenet as img

# silent =" 2>/dev/null || true"

# PROJ_DIR = os.path.realpath(os.path.join(__file__, os.pardir))
# APPS_CONF_DIR = os.path.join(PROJ_DIR, "apps")


# seed()

def parse_args():
    parser = argparse.ArgumentParser()

    def csv_str(str1):
        return str1.split(',')
    parser.add_argument('-t','--testname',dest='tname',default='test',help='Testname | e.g. mytest')
    parser.add_argument('-n','--num',dest='num',default=1,type=int,help='Number of tenants | e.g. 2')
    # parser.add_argument('-m','--master',dest='m_id',required=True,type=csv_str,help='Server | e.g. kvm-1,kvm-2')
    parser.add_argument('-s','--servers',dest='s_id',required=True,type=csv_str,help='Server | e.g. kvm-1,kvm-2')
    parser.add_argument('-c','--clients',dest='c_id',required=True,type=csv_str,help='Clients | e.g. kvm-1,kvm-2')
    parser.add_argument('-d','--device',dest='d_id',default='sdb,sdb,sdb,sdb',type=csv_str,help='Storage | e.g. sdb,sdb')
    parser.add_argument('-mt','--mount',dest='mnt',required=False,type=str,help='Dataset Mountpath | e.g. /mnt/lustre/cosmo')
    parser.add_argument('-g','--gpu',dest='gpu',required=False,help='No. of GPUs')
    parser.add_argument('-proc','--procs',dest='procs',required=False,help='No. of processes or tasks')
    parser.add_argument('-th','--threads',dest='th',required=False,help='Number of threads for Pre-processing')
    parser.add_argument('-ct','--ctrace',dest='ctrace',action='store_true',help='Run Ctrace if flag enabled')
    
    # TODO currently shared account id/password. it needs to change to multiple account and password
    parser.add_argument('-u','--user',dest='user',default='root',help='DevClous Autorun server account | e.g. root')
    parser.add_argument('-p','--password',dest='password',default='password',help='DevCloud Autorun server account | e.g. password')

    args = parser.parse_args()
    return args


def main():
    # Get working directory(absolute)
    cwd = os.getcwd()

    # Collect arguments
    args = parse_args()
    dev = args.d_id
    node = len(args.c_id)
    clients = " ".join(args.c_id)
    gpu = args.gpu
    procs = args.procs
    th = args.th
    master = '192.168.61.88'
    

    
    # Initialize conn in servers
    brokers = [RemoteConn(svr) for svr in args.s_id]
    #Initialize conn in clients
    client_conn = [RemoteConn(svr) for svr in args.c_id]
    # Initialize conn in master
    master_conn = RemoteConn(master)
    
    
    # Init variables
    respath = init_test(args.tname, cwd, f'{cwd}/apps/imagenet/yaml')
    respath_imagenet = mkdir_p(respath,'stage1')
    print("respath :", respath)
    yamlpath = os.path.join(respath, 'yaml')
    ns = f'{args.tname}-imagenet'

    
    # Imagenet Pre-run setup
    pre_imagenet_mkdir = img.Imagenet('imagenet', clients, ns, yamlpath, master_conn)
    pre_imagenet_mkdir.imagenet_load_bm(master_conn)
    
    # Start Imagenet Pre-Processing - Stage 1
    print(f'Loading benchmark.... Load Phase')
    # load_imagenet = img.Imagenet('imagenet', clients, ns, yamlpath)
    # load_imagenet.imagenet_load_bm(master_conn)
    

if __name__=='__main__':
    main()


========

새해 복 많이 받으시고, 부디 건강하시고, 가족 모두 행복한 한해 되시길 기원합니다.

=====================
[root@bright88 democratic-csi]# cat examples/nfs-client.yaml
# driver only works with 1.16+
csiDriver:
  # should be globally unique for a given cluster
  name: "org.democratic-csi.nfs-client"

storageClasses:
- name: nfs-client
  defaultClass: false
  reclaimPolicy: Delete
  volumeBindingMode: Immediate
  allowVolumeExpansion: false
  parameters:
    fsType: nfs

  mountOptions:
  - noatime
  - nfsvers=3
  secrets:
    provisioner-secret:
    controller-publish-secret:
    node-stage-secret:
    node-publish-secret:
    controller-expand-secret:

# if your cluster supports snapshots you may enable below
volumeSnapshotClasses: []
#- name: nfs-client
#  secrets:
#    snapshotter-secret:


driver:
  config:
    # please see the most up-to-date example of the corresponding config here:
    # https://github.com/democratic-csi/democratic-csi/tree/master/examples
    # YOU MUST COPY THE DATA HERE INLINE!
    driver: nfs-client
    instance_id:
    nfs:
      shareHost: 192.168.61.89
      shareBasePath: "/mnt/lustre"
      # shareHost:shareBasePath should be mounted at this location in the controller container
      controllerBasePath: "/mnt/lustre/data"
      dirPermissionsMode: "0777"
      dirPermissionsUser: root
      dirPermissionsGroup: root
    #...

# There are 4 different approaches to installing the driver
# 1: Run the controller service separated from the node service, mount the base share into the controller pod at run time
# 2. Run the controller service separated from the node service, use an existing hostPath mount of the base share from the node in the controller pod
# 3. Run the controller service jointly with the node service, mount the base share into the node pod at run time
# 4. Run the controller service jointly with the node service, use an existing hostPath mount of the base share from the node in the node pod
#
# Uncomment the lines/sections below appropriate for your desired use-case

controller:
  enabled: true

  externalResizer:
    enabled: false

  # For Options 1 and 2
  strategy: deployment

  # For Option 1
  #hostNetwork: true
  #hostIPC: true

  # For Options 3 and 4
  #strategy: node


# Option 1
# do this if the nodes do NOT already have the base volume mounted out-of-band from k8s
#  driver:
#    securityContext:
#      allowPrivilegeEscalation: true
#      capabilities:
#        add:
#        - SYS_ADMIN
#      privileged: true
#    lifecycle:
#      postStart:
#        exec:
#          command: ["/bin/sh", "-c", "mkdir -p <controllerBasePath>; mount <shareHost>:<shareBasePath> <controllerBasePath>"]
#      preStop:
#        exec:
#          command: ["/bin/sh","-c","umount <controllerBasePath>"]

# Option 2
# do this if all nodes DO have the base volume mounted out-of-band from k8s
  driver:
    securityContext:
      allowPrivilegeEscalation: true
      capabilities:
        add:
        - SYS_ADMIN
      privileged: true

    extraVolumeMounts:
    - name: nfs-storage
      mountPath: /mnt/lustre/data
      mountPropagation: Bidirectional
#
  extraVolumes:
  - name: nfs-storage
    hostPath:
      path: /mnt/lustre
      type: Directory


# Options 3 and 4
#node:

# Option 3
# do this if the nodes do NOT already have the base volume mounted out-of-band from k8s
#  driver:
#    lifecycle:
#      postStart:
#        exec:
#          command: ["/bin/sh", "-c", "mkdir -p <controllerBasePath>; mount <shareHost>:<shareBasePath> <controllerBasePath>"]
#      preStop:
#        exec:
#          command: ["/bin/sh","-c","umount <controllerBasePath>"]

# Option 4
# do this if all nodes DO have the base volume mounted out-of-band from k8s
#  driver:
#    extraVolumeMounts:
#    - name: nfs-storage
#      mountPath: <controllerBasePath>
#      mountPropagation: Bidirectional
#
#  extraVolumes:
#  - name: nfs-storage
#    hostPath:
#      path: /already/mounted/path/to <shareHost>:<shareBasePath>
#      type: Directory




=====
apiVersion: v1
kind: PersistentVolume
metadata:
  annotations:
    pv.kubernetes.io/provisioned-by: org.democratic-csi.nfs-client
  creationTimestamp: "2023-01-20T02:25:38Z"
  finalizers:
  - kubernetes.io/pv-protection
  name: pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26
  resourceVersion: "19593411"
  uid: d87f17ae-88bb-4a2d-8666-2fdc96adb1f6
spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 200Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: kf-claim-nfs-2
    namespace: default
    resourceVersion: "19593405"
    uid: 25f0d0be-41de-4a4d-ba21-e39088b86d26
  csi:
    controllerExpandSecretRef:
      name: controller-expand-secret-nfs-client-zfs-nfs-democratic-csi
      namespace: democratic-csi
    controllerPublishSecretRef:
      name: controller-publish-secret-nfs-client-zfs-nfs-democratic-csi
      namespace: democratic-csi
    driver: org.democratic-csi.nfs-client
    fsType: nfs
    nodePublishSecretRef:
      name: node-publish-secret-nfs-client-zfs-nfs-democratic-csi
      namespace: democratic-csi
    nodeStageSecretRef:
      name: node-stage-secret-nfs-client-zfs-nfs-democratic-csi
      namespace: democratic-csi
    volumeAttributes:
      node_attach_driver: nfs
      provisioner_driver: nfs-client
      server: 192.168.61.89
      share: /mnt/lustre/v/pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26
      storage.kubernetes.io/csiProvisionerIdentity: 1673590679347-8081-org.democratic-csi.nfs-client
    volumeHandle: pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26
  mountOptions:
  - noatime
  - nfsvers=3
  persistentVolumeReclaimPolicy: Delete
  storageClassName: nfs-client
  volumeMode: Filesystem
status:
  phase: Bound



===============================================
apiVersion: v1
kind: Pod
metadata:
  name: mlperf
spec:
  nodeSelector:
    kubernetes.io/hostname: node001
  containers:
  - name: mlperf
    image: 192.168.61.4:5000/nvidia_rn50_mx:0.2
    imagePullPolicy: IfNotPresent
    securityContext:
      privileged: true
    command: ["/bin/sh", "-c"]
    args:
        - "ls /data/imagenet/train-val-recordio-passthrough/"
          #        - "./scripts/prepare_imagenet.sh /data/imagenet/train-val-recordio-passthrough/tiny-imagenet-200 /data/imagenet/train-val-recordio-passthrough/tiny-imagenet-200 40"
    volumeMounts:
    - name: mlperf-volume
      mountPath: /data/imagenet/train-val-recordio-passthrough
  volumes:
  - name: mlperf-volume
    persistentVolumeClaim:
      claimName: kf-claim-nfs-2
  imagePullSecrets:
  - name: regcred





kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: kf-claim-nfs-2
  annotations:
    volume.beta.kubernetes.io/storage-class: "nfs-client"
spec:
  storageClassName: nfs-client
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 200Gi
  nodeSelector:
    kubernetes.io/hostname: node001







[root@bright88 test-scripts]# kubectl describe pv pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26
Name:            pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26
Labels:          <none>
Annotations:     pv.kubernetes.io/provisioned-by: org.democratic-csi.nfs-client
Finalizers:      [kubernetes.io/pv-protection]
StorageClass:    nfs-client
Status:          Bound
Claim:           default/kf-claim-nfs-2
Reclaim Policy:  Delete
Access Modes:    RWO
VolumeMode:      Filesystem
Capacity:        200Gi
Node Affinity:   <none>
Message:
Source:
    Type:              CSI (a Container Storage Interface (CSI) volume source)
    Driver:            org.democratic-csi.nfs-client
    FSType:            nfs
    VolumeHandle:      pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26
    ReadOnly:          false
    VolumeAttributes:      node_attach_driver=nfs
                           provisioner_driver=nfs-client
                           server=192.168.61.89
                           share=/mnt/lustre/v/pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26
                           storage.kubernetes.io/csiProvisionerIdentity=1673590679347-8081-org.democratic-csi.nfs-client
Events:                <none>
[root@bright88 test-scripts]# kubectl  get pvc
NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
kf-claim-nfs-2   Bound    pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26   200Gi      RWO            nfs-client     7m15s
[root@bright88 test-scripts]# kubectl describe pvc kf-claim-nfs-2
Name:          kf-claim-nfs-2
Namespace:     default
StorageClass:  nfs-client
Status:        Bound
Volume:        pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26
Labels:        <none>
Annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
               volume.beta.kubernetes.io/storage-class: nfs-client
               volume.beta.kubernetes.io/storage-provisioner: org.democratic-csi.nfs-client
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      200Gi
Access Modes:  RWO
VolumeMode:    Filesystem
Used By:       mlperf
Events:
  Type    Reason                 Age                    From                                                                                                                  Message
  ----    ------                 ----                   ----                                                                                                                  -------
  Normal  Provisioning           7m34s                  org.democratic-csi.nfs-client_zfs-nfs-democratic-csi-controller-f7557ff5d-m6wjc_d73e305d-5952-4ae2-9ba3-b9a36a67df4b  External provisioner is provisioning volume for claim "default/kf-claim-nfs-2"
  Normal  ProvisioningSucceeded  7m34s                  org.democratic-csi.nfs-client_zfs-nfs-democratic-csi-controller-f7557ff5d-m6wjc_d73e305d-5952-4ae2-9ba3-b9a36a67df4b  Successfully provisioned volume pvc-25f0d0be-41de-4a4d-ba21-e39088b86d26
  Normal  ExternalProvisioning   7m33s (x2 over 7m33s)  persistentvolume-controller                                                                                           waiting for a volume to be created, either by external provisioner "org.democratic-csi.nfs-client" or manually created by system administrator





[root@bright88 examples]# kubectl get pods -n democratic-csi
NAME                                                READY   STATUS    RESTARTS   AGE
zfs-nfs-democratic-csi-controller-f7557ff5d-m6wjc   4/4     Running   169        7d15h
zfs-nfs-democratic-csi-node-47vf2                   4/4     Running   0          13d
zfs-nfs-democratic-csi-node-8b7pp                   4/4     Running   461        13d
zfs-nfs-democratic-csi-node-bfzpm                   4/4     Running   0          13d
zfs-nfs-democratic-csi-node-d4pqk                   4/4     Running   0          13d




driver: lustre-client
instance_id:
lustre:
  shareHost: 192.168.61.80
  shareBasePath: "/mnt/lustre"
  # shareHost:shareBasePath should be mounted at this location in the controller container
  controllerBasePath: "/storage"
  dirPermissionsMode: "0777"
  dirPermissionsUser: root
  dirPermissionsGroup: root






MountVolume.MountDevice failed for volume "pvc-7b5b1f73-d73a-4249-bda2-45524313d69a" : rpc error: code = Internal desc = {"code":32,"stdout":"","stderr":"/usr/local/bin/mount: illegal option -- o\nmount.nfs: requested NFS version or transport protocol is not supported\n","timeout":false}

==============================================

Volumes:
  mlperf-volume:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  mlperf-test-node001-pvc
    ReadOnly:   false
  mlperf-log:
    Type:          HostPath (bare host directory volume)
    Path:          /mnt/
    HostPathType:
  host-folder:
    Type:          HostPath (bare host directory volume)
    Path:          /tmp/
    HostPathType:
  kube-api-access-hmb84:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/hostname=node001
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age               From               Message
  ----     ------            ----              ----               -------
  Warning  FailedScheduling  18s               default-scheduler  0/7 nodes are available: 7 pod has unbound immediate PersistentVolumeClaims.
  Normal   Scheduled         17s               default-scheduler  Successfully assigned mlperf-test-node001/mlperf to node001
  Warning  FailedMount       1s (x6 over 17s)  kubelet            MountVolume.MountDevice failed for volume "pvc-fd4bde75-15bd-4962-b2c7-8d2232d0bfb2" : rpc error: code = Internal desc = {"code":32,"stdout":"","stderr":"/usr/local/bin/mount: illegal option -- o\nmount.nfs: requested NFS version or transport protocol is not supported\n","timeout":false}




error parsing load.yaml: error converting YAML to JSON: yaml: line 20: did not find expected key



from kubernetes import client, config

class KubernetesWorker:
    def __init__(self):
        # loading kubeconfig
        config.load_kube_config()
        # create an instance of the Kubernetes CoreV1Api
        self.api_instance = client.CoreV1Api()

    def show_nodes(self):
        nodes = self.api_instance.list_node()
        for node in nodes.items:
            print(node.metadata.name)

# Usage
worker = KubernetesWorker()
worker.show_nodes()












slurmstepd: error: pyxis: container start failed with error code: 1
slurmstepd: error: pyxis: printing enroot log file:
slurmstepd: error: pyxis:     /usr/bin/enroot: line 44: HOME: unbound variable
slurmstepd: error: pyxis:     [ERROR] Command not found: nvidia-container-cli, see https://github.com/NVIDIA/libnvidia-container
slurmstepd: error: pyxis:     [ERROR] /etc/enroot/hooks.d/98-nvidia.sh exited with return code 1















[root@bright88 mxnet]# srun --mpi=pmix_v3 -N 1 --ntasks=16  -w node001 --container-image=192.168.61.4:5000#/cosmoflow-nvidia:0.4 --container-name=cosmoflow-preprocess --container-workdir=/mnt/mxnet --container-mounts=/mnt/lustre:/mnt bash tools/init_datasets.sh  /mnt/Cosmo-Small /mnt/processed 16









pyxis: imported docker image: 192.168.61.4:5000#/cosmoflow-nvidia:0.4
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
dpkg: warning: version '4.18.0-372.9.1.el8.x86_64' has bad syntax: invalid character in revision number
[node001:110631] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110633] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110638] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110636] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110640] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110639] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110632] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110642] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110641] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110634] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110644] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110635] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110643] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110630] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110629] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
[node001:110637] PMIX ERROR: ERROR in file gds_ds12_lock_pthread.c at line 168
2023-01-11 15:37:23.068623: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.068624: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.070205: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.072718: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.080399: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.085484: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.085311: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.086233: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.086883: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.088807: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.092000: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.094736: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.097027: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.098812: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.220895: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:23.228529: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-11 15:37:32.585366: F tensorflow/core/platform/statusor.cc:33] Attempting to fetch value instead of handling error INTERNAL: failed initializing StreamExecutor for CUDA device ordinal 0: INTERNAL: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 85197979648
[node001:110642] *** Process received signal ***
[node001:110642] Signal: Aborted (6)
[node001:110642] Signal code:  (-6)
[node001:110642] [ 0] /usr/lib/x86_64-linux-gnu/libc.so.6(+0x46210)[0x155555370210]
[node001:110642] [ 1] /usr/lib/x86_64-linux-gnu/libc.so.6(gsignal+0xcb)[0x15555537018b]
[node001:110642] [ 2] /usr/lib/x86_64-linux-gnu/libc.so.6(abort+0x12b)[0x15555534f859]
[node001:110642] [ 3] /usr/local/lib/python3.8/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(+0x10a461a4)[0x1555331541a4]
[node001:110642] [ 4] /usr/local/lib/python3.8/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(_ZN10tensorflow17internal_statusor6Helper5CrashERKNS_6StatusE+0x4e)[0x1555331477ce]
[node001:110642] [ 5] /usr/local/lib/python3.8/dist-packages/tensorflow/python/../libtensorflow_framework.so.2(+0x9b3f0b)[0x155521133f0b]
[node001:110642] [ 6] /usr/local/lib/python3.8/dist-packages/tensorflow/python/../libtensorflow_framework.so.2(_ZN10tensorflow20BaseGPUDeviceFactory13CreateDevicesERKNS_14SessionOptionsERKSsPSt6vectorISt10unique_ptrINS_6DeviceESt14default_deleteIS8_EESaISB_EE+0x77b)[0x15552113a95b]
[node001:110642] [ 7] /usr/local/lib/python3.8/dist-packages/tensorflow/python/../libtensorflow_framework.so.2(_ZN10tensorflow13DeviceFactory10AddDevicesERKNS_14SessionOptionsERKSsPSt6vectorISt10unique_ptrINS_6DeviceESt14default_deleteIS8_EESaISB_EE+0xad)[0x155520ea5bfd]
[node001:110642] [ 8] /usr/local/lib/python3.8/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so(TFE_NewContext+0x70)[0x155526cb6050]
[node001:110642] [ 9] /usr/local/lib/python3.8/dist-packages/tensorflow/python/_pywrap_tfe.so(+0x598f5)[0x15551fb948f5]
[node001:110642] [10] /usr/local/lib/python3.8/dist-packages/tensorflow/python/_pywrap_tfe.so(+0x55141)[0x15551fb90141]
[node001:110642] [11] python3(PyCFunction_Call+0x59)[0x5f2cc9]
[node001:110642] [12] python3(_PyObject_MakeTpCall+0x150)[0x5f3010]
[node001:110642] [13] python3(_PyEval_EvalFrameDefault+0x5d43)[0x5700f3]
[node001:110642] [14] python3(_PyFunction_Vectorcall+0x1b6)[0x5f5956]
[node001:110642] [15] python3(_PyEval_EvalFrameDefault+0x906)[0x56acb6]
[node001:110642] [16] python3(_PyEval_EvalCodeWithName+0x26a)[0x568d9a]
[node001:110642] [17] python3(_PyFunction_Vectorcall+0x393)[0x5f5b33]
[node001:110642] [18] python3(_PyEval_EvalFrameDefault+0x72f)[0x56aadf]
[node001:110642] [19] python3(_PyFunction_Vectorcall+0x1b6)[0x5f5956]
[node001:110642] [20] python3(_PyEval_EvalFrameDefault+0x72f)[0x56aadf]
[node001:110642] [21] python3(_PyEval_EvalCodeWithName+0x26a)[0x568d9a]
[node001:110642] [22] python3(_PyFunction_Vectorcall+0x393)[0x5f5b33]
[node001:110642] [23] python3(_PyEval_EvalFrameDefault+0x18eb)[0x56bc9b]
[node001:110642] [24] python3(_PyEval_EvalCodeWithName+0x26a)[0x568d9a]
[node001:110642] [25] python3(_PyFunction_Vectorcall+0x393)[0x5f5b33]
[node001:110642] [26] python3(_PyEval_EvalFrameDefault+0x18eb)[0x56bc9b]
[node001:110642] [27] python3(_PyEval_EvalCodeWithName+0x26a)[0x568d9a]
[node001:110642] [28] python3(_PyFunction_Vectorcall+0x393)[0x5f5b33]
[node001:110642] [29] python3(_PyEval_EvalFrameDefault+0x18eb)[0x56bc9b]
[node001:110642] *** End of error message ***
tools/init_datasets.sh: line 7: 110642 Aborted                 (core dumped) python3 -m tools.convert_tfrecord_to_numpy -i ${DATA_SRC_DIR}/train -o ${DATA_DST_DIR}/train -c GZIP -p ${NUM_PROC}
Train set done
slurmstepd: error:  mpi/pmix_v3: _errhandler: node001 [0]: pmixp_client_v2.c:211: Error handler invoked: status = -25, source = [slurm.pmix.259.0:7]
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 259.0 ON node001 CANCELLED AT 2023-01-11T15:37:34 ***
slurmstepd: error:  mpi/pmix_v3: _errhandler: node001 [0]: pmixp_client_v2.c:211: Error handler invoked: status = -25, source = [slurm.pmix.259.0:9]
slurmstepd: error:  mpi/pmix_v3: _errhandler: node001 [0]: pmixp_client_v2.c:211: Error handler invoked: status = -25, source = [slurm.pmix.259.0:10]
srun: error: node001: tasks 0-6,8-15: Killed
